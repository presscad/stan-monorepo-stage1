\documentclass[10pt]{report}

%\usepackage{times}
\usepackage[%romanfamily=times,
            scale=0.875,
            stdmathitalics=true,
            stdmathdigits=true]{lucimatx}
\usepackage{xspace}
\usepackage{fancyvrb}
\usepackage{titlesec}

\titleformat{\chapter}[hang]{\bfseries\huge}{\thechapter.}{1.5pc}{}{}
%\titlespacing{\chapter}{0pt}{-12pt}{18pt}{}
%
\titleformat{\section}{\bfseries\large}{\thesection.}{1em}{}
%\titlespacing{\section}{0pt}{12pt}{6pt}
%
\titleformat{\subsection}{\it}{}{0em}{}
%\titlespacing{\subsection}{-1em}{12pt}{6pt}

\newcommand{\Stan}{Stan\xspace}
\newcommand{\stanc}{{\ttfamily stanc}\xspace}
\newcommand*{\Cpp}{C\raise.2ex\hbox{\footnotesize ++}\xspace} %\ensuremath{++}
\newcommand{\clang}{{\ttfamily clang\raise.2ex\hbox{\footnotesize ++}}\xspace} 
\newcommand{\gpp}{{\ttfamily g\raise.2ex\hbox{\footnotesize ++}}\xspace} 

\newcommand{\acronym}[1]{{\sc #1}\xspace}

\newcommand{\R}{\acronym{r}}
\newcommand{\SPLUS}{\acronym{s}}
\newcommand{\BUGS}{\acronym{bugs}}
\newcommand{\JAGS}{\acronym{jags}}
\newcommand{\MCMC}{\acronym{mcmc}}
\newcommand{\HMC}{\acronym{hmc}}
\newcommand{\NUTS}{\acronym{nuts}}
\newcommand{\MSVC}{\acronym{msvc}}
\newcommand{\LKJ}{\acronym{lkj}}
\newcommand{\CPC}{\acronym{cpc}}

\newcommand{\code}[1]{{\tt #1}}
\newcommand{\mycaption}[2]{\caption{{\it #2}\label{#1.figure}}}

\newcommand{\refappendix}[1]{Appendix~\ref{#1.appendix}}
\newcommand{\refchapter}[1]{Chapter~\ref{#1.chapter}}
\newcommand{\reffigure}[1]{Figure~\ref{#1.figure}}

\newcommand{\fitem}[4]{\item[{\tt #1 {\bfseries #2}(#3)}]\mbox{ } \\[4pt] #4}
\newcommand{\farg}[1]{{\tt\slshape #1}}

\usepackage{url}

\newcommand\thicktilde

\newcommand{\samp}{{\lower.78ex\hbox{\texttt{\char`\~}}}}%
%{{\lower1.1ex\hbox{\Large\texttt{\char`\~}}}}

\setcounter{tocdepth}{0}

\title{\Huge\bf \Stan Modeling Language}
\author{\Stan's Development Team}
\date{\vspace*{36pt} Version 1.0 \\[4pt] {\small \today}} % \footnotesize \today}

\begin{document}

\maketitle

\begin{abstract}
  \Stan is a general-purpose probabilistic modeling framework designed
  to support full Bayesian inference.  This document describes \Stan's
  modeling language for specifying the joint probability of observed
  data and unobserved parameters.  \Stan's compiler parses model
  specifications and generates \Cpp code.  \Stan's modeling language
  is based on \BUGS and \JAGS, and like them, allows users to develop,
  fit, and evaluate Bayesian models without knowing \Cpp.

  \Stan performs inference by Markov chain Monte Carlo (\MCMC)
  sampling of parameter values from the posterior distribution of
  parameters given the observed data.  \Stan employs the no-U-turn
  sampler (\NUTS), an adaptive form of Hamiltonian Monte Carlo (\HMC)
  that alleviates the need for user tuning of \HMC's rather sensitive
  parameters.  By making effective use of the gradient of the log
  posterior, \HMC converges and explores the parameter distribution
  faster than Gibbs sampling or random-walk Metropolis-Hastings.  The
  improved sampling algorithm and more compact and efficient compiled
  code allows \Stan to operate on data of larger scales and models of
  more complex structure than \BUGS or \JAGS.
\end{abstract}

\tableofcontents

\part{Introduction}

\chapter{What is \Stan?}

This document is a reference manual and getting started guide for
using \Stan's probabilistic modeling language.  After describing the
overall system in this introduction and providing a hands-on
quick-start guide in the following chapter, the remainder of the
document is devoted to fully documenting the behavior of
Stan's modeling language.

\Stan's modeling language and its execution behavior are similar to
that of its progenitors, \BUGS and \JAGS.  It differs in many
particulars of the modeling language, which is more like an
impertative programming language than the declarative specifications
of \BUGS and \JAGS.

\section{\Stan's Modeling Language}

\noindent
\Stan's modeling language allows users to code a Bayesian model
specifying a joint probability function
\[
p(y,\theta),
\]
where 
\begin{itemize}
\item
  $y$ is a vector of known values, such as 
  constants, hyperparameters, and modeled data, and
\item
 $\theta$ is a vector of unknown values, such as estimated parameters,
  missing data, and simulated values.
\end{itemize}
%
To simplify terminology, $y$ will be called the data vector and
$\theta$ the parameter vector.  The probability function $p(y,\theta)$
need only be specified up to a multiplicative constant with respect to
any fixed data vector $y$.  This ensures proportionality of the
posterior to the specified joint probability,
\[
p(\theta|y) \propto p(y,\theta) = p(\theta|y) \, p(y).
\]

Stan's language is more imperative than its declarative 
predecessors, \BUGS and \JAGS.  Statements are executed in the order 
they are specified and variables and expressions are strongly typed 
and declared as data or parameters in the model rather than by a 
calling function.  Stan also supports a broader range of arithmetic, 
matrix, and linear algebra operations than \BUGS or \JAGS.  Users may 
manipulate log probability functions directly and are not required 
to use proper priors.

\section{\Stan's Compiler}

\Stan's compiler, \stanc, reads a user program in \Stan's modeling
language and generates a \Cpp class implementing the model specified
by that program.  \Stan automatically applies a multivariate transform
(and its Jacobian determinant) to free any constrained parameters,
such as deviations (constrained to be positive), simplexes (a vector
constrained to be positive and to sum to 1), and covariance matrices
(positive definiteness).  The result is an unconstrained sampling (or
optimization) problem from the perspective of the sampler.  From the
user's perspective, this transform happens behind the scenes, driven
by the types declared for each of the parameters.

The generated \Cpp class can be plugged into Stan's continuous and
discrete samplers to read the data $y$ and then draw a sequence of
sample parameter vectors $\theta^{(m)}$ according to the posterior,
\[
p(\theta|y) = \frac{p(y,\theta)}{p(y)} \propto p(y,\theta).
\]
The resulting samples may be used for full Bayesian inference, much of
which can be carried out within Stan.

\section{\Stan's Samplers}

For continuous variables, Stan uses Hamiltonian Monte Carlo (\HMC)
sampling. \HMC is a Markov chain Monte Carlo (\MCMC) method based on
simulating the Hamiltonian dynamics of a fictional physical system in
which the parameter vector $\theta$ represents the position of a
particle in $K$-dimensional space and potential energy is defined to
be the negative (unnormalized) log probability.  Each sample in the
Markov chain is generated by starting at the last sample, applying a
random momentum to determine initial kinetic energy, then simulating
the path of the particle in the field.  Standard \HMC runs the
simulation for a fixed number of discrete steps of a fixed step size
and uses a Metropolis adjustment to ensure detailed balance of the
resulting Markovian system.  This adjustment treats the momentum term
of the Hamiltonian as an auxiliary variable, and the only reason for
rejecting a sample will be discretization error in computing the
Hamiltonian.

\HMC treats the position of a particle, 
log probability as a negative potential
energy function, then samples by adding random kinetic energy and
simulating the 

In addition to basic \HMC, Stan implements an adaptive
version of \HMC, the No-U-Turn Sampler (\NUTS).  \NUTS automatically
tunes step sizes and a diagonal mass matrix during warmup and then
adapts the number of leapfrog integration steps during sampling.
Stan is expressive enough to allow most discrete variables to be
marginalized out.  For the remaining discrete parameters, Stan uses
Gibbs sampling if there are only a few outcomes and adaptive slice
sampling otherwise.


\chapter{Getting Started}

This chapter is designed to help users get acquainted with the overall
design of the \Stan language and calling \Stan from the command line.
For installation information, see \refappendix{install}.
Later chapters are devoted to expanding on the material in this
chapter with full reference documentation.


\section{A Minimal Program}

Stan is distributed with several working models.  The simplest of
these is found in the following location relative to the top-level
distribution.
%
\begin{quote}
\begin{Verbatim}
src/models/basic_distributions/normal.stan
\end{Verbatim}
\end{quote}
%
The contents of this file are as follows.
%
\begin{quote}
\begin{Verbatim}
parameters {
  real y;
}
model {
  y ~ normal(0,1);
}
\end{Verbatim}
\end{quote}
%
The model's single parameter \code{y} is declared to take real values.
The probability model specifies that \code{y} has a normal
distribution with location 0 and scale 1.  Basically, this model will
sample a single unit normal variate.  

\section{Whitespace and Semicolons}

In Stan, every variable declaration and atomic statement must be
terminated by a semicolon (\code{;}).  This is the convention followed
by programming languages such as \Cpp.  It is not the convention
followed by the statistical languages \R, \BUGS, or \JAGS.

The reason for the \Cpp convention is to ensure that differences in
whitespace are not meaningful.  In \R, \BUGS, and \JAGS, the following
is a complete, legal statement.
%
\begin{quote}
\begin{Verbatim}
a <- b +
     c
\end{Verbatim}
\end{quote}
%
In contrast, the usual way of typesetting mathematics and laying out
code in programming languages, with the operator continuing the
expression beginning a new line, is invalid.
%
\begin{quote}
\begin{Verbatim}
a <- b
     + c
\end{Verbatim}
\end{quote}
%
The only difference is in the kind of whitespace between \code{b} and
\code{+} and between \code{+} and \code{c}.  In \Stan, there is no
whitespace-dependent behavior.  Neither of these is a complete
statement, whereas either one terminated with a semicolon is.  The
second form is recommended for \Cpp and \Stan.


\section{Compiling  with {\tt\bfseries stanc}}

Starting at Stan's home directory, written here as {\tt \$stan},
the model may be compiled by the Stan compiler, \stanc, into \Cpp code
as follows.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd $stan
> stanc src/models/basic_distributions/normal.stan
\end{Verbatim}
%
\begin{Verbatim}
Model name=anon_model
Input file=src/models/basic_distributions/normal.stan
Output file=anon_model.cpp
\end{Verbatim}
\end{quote}
%
The output indicates the name of the model, here the default value
\code{anon\_model}, the input file from which the Stan program is
read, here \code{normal.stan}, and the output file to which the
generated \Cpp code is written, here \code{anon\_model.cpp}.  See
\refchapter{stanc} for more documentation on the \stanc compiler.

\section{Compiling the Generated Code}

The file generated by \stanc must next be compiled with a \Cpp
compiler by linking to \Stan's source and library directories using
the {\tt -I} option of the compiler.  The following example 
uses the \clang compiler for \Cpp.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> clang++ -I src -I lib anon_model.cpp 
\end{Verbatim}
\end{quote}
%
This command invokes the \clang compiler for \Cpp to create a
platform-specific executable in the default location, which is {\tt
  a.out} by convention.  If all goes well, as above, there is no
output to the console.  More information about compiling the \Cpp code
generated by Stan may be found in \refchapter{compiling-cpp}.
Installation information for \Cpp compilers may be found in
\refappendix{install}.

\section{Running the Sampler}

The executable resulting from compiling the generated \Cpp may be run
as follows.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./a.out
\end{Verbatim}
%
\begin{Verbatim}
STAN SAMPLING COMMAND
data = 
init = random initialization
samples = samples.csv
append_samples = 0
seed = 1331941513 (randomly generated)
chain_id=1 (default)
iter = 2000
warmup = 1000
thin = 1
leapfrog_steps = -1
max_treedepth = 10
epsilon = -1
epsilon_pm = 0
epsilon_adapt_off = 0
delta = 0.5
gamma = 0.05

Iteration: 2000 / 2000 [100%]  (Sampling)
\end{Verbatim}
\end{quote}
%
The program indicates to the standard output that the samples are
written to \code{samples.csv}.  The first few lines of this file
are comments about aspects of the run.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cat samples.csv
\end{Verbatim}
\begin{Verbatim}
# Samples Generated by Stan
#
# stan_version_major=alpha
# stan_version_minor=0
# data=
# init=random initialization
# append_samples=0
# seed=1331941796
# chain_id=1
# iter=2000
# warmup=1000
# thin=1
# leapfrog_steps=-1
# max_treedepth=10
# epsilon=-1
# epsilon_pm=0
# delta=0.5
# gamma=0.05
...
\end{Verbatim}
\end{quote}
%
The ellipses notation, {\tt ...}, indicates that the output continues
beyond what's shown.  Here, what follows is the data in standard
comma-separate value ({\sc csv}) notation.
%
\begin{quote}
\begin{Verbatim}
...
lp__,treedepth__,y
-0.0126699,1,0.159185
-0.222796,1,-0.667527
-0.222796,1,-0.667527
-0.404457,1,-0.899397
...
\end{Verbatim}
\end{quote}
%
The first line consists of a header indicating the names of the
variables on the lines to follow, and each following line indicates a
single sampled value of the parameters.  The first column is reserved
for the (unnormalized) log probability (density) of the parameters,
with name {\tt lp\_\_} (the underscores are to prevent name conflicts
with user-defined model parameters).  The next values are for
reporting the behavior of the sampler.  In this case, the \NUTS
sampler was used, so there is a report of the depth of tree it
explored, with variable name {\tt treedepth\_\_}.  The remaining
values are parameters.  Here, the model has only one parameter, {\tt
  y}.  The first sampled value for {\tt y} is 0.159185, the second is
-0.667527, and so on.  

Note that the second sampled value is repeated.  This is not a bug.
Rather, it is the behavior to expect from a sampler using a Metropolis
acceptance step for proposals, as Stan's samplers \HMC and \NUTS do.

\section{Data}

\Stan allows data to be specified in programs, used in models, and
read into compiled \Stan programs. This section provides an example of
coding and running a \Stan program with data stored in a file in the
\SPLUS/\R dump format.

The Stan program in 
\begin{quote}
\begin{Verbatim}
src/models/basic_estimators/bernoulli.stan
\end{Verbatim}
\end{quote}
can be used to estimate a Bernoulli parameter \code{theta} from
\code{N} binary observations.  The file contains the following code.
%
\begin{quote}
\begin{Verbatim}
data {
  int(0,) N;
  int(0,1) y[N];
}
parameters {
  real(0,1) theta;
}
model {
  theta ~ beta(1,1);
  for (n in 1:N)
    y[n] ~ bernoulli(theta);
}
\end{Verbatim}
\end{quote}
%
This program declares two data variables in its \code{data} block.
The first data variable, \code{N}, is an integer encoding the number
of observations.  The declaration \code{int(0,)} indicates that
\code{N} must take on non-negative values.  The second data variable,
\code{y}, is declared as \code{y[N]}, specifying that it is an array
of \code{N} values.  Each of these values has the declared type,
\code{int(0,1)}, an integer between 0 and 1 inclusive, i.e., a binary
value.  The \code{N} individual binary values in the array \code{y}
are accessed using standard array notation, indexing from 1, as \code{y[1]},
\code{y[2]}, ..., \code{y[N]}.

The \code{parametes} block declares a single parameter, \code{theta}.
Its type is given as \code{real(0,1)}, meaning it takes on continuous
values between 0 and 1 inclusive.  The constraint is necessary in
order to ensure that \code{theta} takes on a legal value as the
success parameter in the Bernoulli distribution in which it is used in
the \code{model} block of the program.

The \code{model} block consists of a for-loop for the data.   The loop is
specified so that the body is executed for values of \code{n} between
\code{1} and \code{N} inclusive.  The body here is a sampling
statement specifying that the variable \code{y[n]} is modeled as
having a Bernoulli distribution with parameter \code{theta}.  

A sample data file for this program can be found in the file
\code{bernoulli.Rdata} in the same directory.  This data file has
the following contents.
%
\begin{quote}
\begin{Verbatim}
N <- 10
y <- c(0,1,0,0,0,0,0,0,0,1)
\end{Verbatim}
\end{quote}
%
A data file must contain appropriate values for all of the data
variables declared in the \Stan program's \code{data} block.  Here there
is a non-negative integer value for \code{N} and an array of length
\code{N} (i.e., 10) integer values between 0 and 1 inclusive.  The
array is coded using the \SPLUS sequence notation \code{c(...)}.
The dump format supported by \Stan is documented in \refchapter{dump}.

The program is compiled by \stanc and the \Cpp compiler in the same
way.  This time, the output model gets an explicitly specified name.
%
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> stanc --name=bern src/models/basic_estimators/bernoulli.stan 
\end{Verbatim}
\begin{Verbatim}
Model name=bern
Input file=src/models/basic_estimators/bernoulli.stan
Output file=bern.cpp
\end{Verbatim}
\end{quote}
%
As before, the \Cpp compiler needs to be given the name of
generated file.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> clang++ -O3 -I src -I lib -o bern bern.cpp
\end{Verbatim}
\end{quote}
%
There are two new compiler options here.  The option \code{-O3} sets
optimization to level 3, which generates much faster executable
code at the expense of slower compilation.  The name of the
executable is also specified, using the option \code{-o~bern}.  Now
the code may be executed by calling its executable with the data file
specified. 
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bern --data=src/models/basic_estimators/bernoulli.Rdata
\end{Verbatim}
\end{quote}



\section{Proper and Improper Priors}

The model in the previous section does not contain a sampling
statement for \code{theta}.  The default behavior is to give
\code{theta} a uniform prior.  In this case, a uniform prior is proper
because \code{theta} is bounded to a finite interval.  Improper priors
are also allowed in \Stan programs; they arise from unconstrained
parameters without sampling statements.  The uniform prior could have
also been added explicitly by adding the following statement to the
\code{model} block of the program.
%
\begin{quote}
\begin{Verbatim} 
theta ~ uniform(0,1);
\end{Verbatim}
\end{quote}
% 
A third way to specify that \code{theta} has a uniform distribution
between 0 and 1 is with the beta distribution.
%
\begin{quote}
\begin{Verbatim}
theta ~ beta(1,1);
\end{Verbatim}
\end{quote}
%
The beta distribution is conjugate to the Bernoulli, but \Stan (at
least as of yet) does not make use of this information.  On the other hand,
these three approaches, no prior, uniform prior, and beta prior,
are equally efficient in \Stan's sampler, because their uniformity
can be determined at compile time and thus computations related to
them eliminated.  There is further discussion of \Stan optimization
in \refchapter{optimization}


\part{Commands and Data Formats}

\chapter{Compiling Stan Programs to C++}\label{stanc.chapter}

Preparing a \Stan program to be run involves two compilation steps,
%
\begin{enumerate}
\item compiling the \Stan program to \Cpp, and
\item compiling the resulting \Cpp to an executable.
\end{enumerate}
%
This chapter discusses the first step; the second step is discussed in
\refchapter{compiling-cpp}.

\section{The \stanc Compiler}

The \stanc compiler converts \Stan programs to \Cpp programs.  

The first thing it does is parse the \Stan program.  If the parser is
successful, it then generates \Cpp code.  If the parser fails, it will
provide an error message indicating where and why the error occurred.

The following example illustrates a fully qualified call to \stanc.
%
\begin{verbatim}
> stanc --name=binary_normal --o=binorm.cpp binormal.stan 
\end{verbatim}
%
This call specifies the name of the model, here {\tt binary\_normal}.
This will determine the name of the class implementing the model in
the \Cpp code.  The \Cpp code implementing the class is written to
\code{binorm.cpp}.  The final argument, \code{binormal.stan}, is
the file from which to read the \Stan program.


\section{Command-Line Options}

\begin{description}
%
\item[\tt {-}-help] 
\mbox{ } \\ 
Displays the manual page for \stanc.  If this option is selected,
nothing else is done.
%
\item[\tt {-}-version]
\mbox{ } \\ 
Prints the version of \stanc.  This is useful for bug reporting
and asking for help on the mailing lists.
%
\item[\tt {-}-name={\slshape class\_name}]
\mbox{ } \\ 
Specify the name of the class used for the implementation of the
\Stan model in the generated \Cpp code.  
\\[6pt]
Default: {\tt {\slshape class\_name = anon\_model}}
%
\item[\tt {-}-o={\slshape cpp\_file\_name}]
\mbox{ } \\ 
Specify the name of the file into which the generated \Cpp is written.
\\[6pt]
Default: {\tt {\slshape cpp\_file\_name} = {\slshape class\_name}.cpp}
%
\end{description}





\chapter{Compiling C++ Programs}\label{compiling-cpp.chapter}

\Stan has been developed using two portable, open-source compilers,
\gpp and \clang, which run under Windows, Macintosh, and Unix/Linux.
\Stan has also been compiled using \MSVC, a Windows-specific compiler
from Microsoft.


\section{Which Compiler?}

It has been our experience that \clang is much faster to
compile at all optimization levels than \gpp, but that the code
generated by \gpp is slightly faster to execute.

\section{What the Compiler Does}

A \Cpp compiler like \gpp or \clang actually performs several
lower-level operations in sequence,
% 
\begin{enumerate}
\item
parsing the input \Cpp source file(s), 
\item 
generating relocatable object code, and
\item 
linking the relocatable object code into executable code.
\end{enumerate}
%
These stages may be called separately, though the examples in
this manual perform them in a single call.


\section{Including Library Code}

\Stan is written as a set of header-only libraries.  This simplifies
writing code that uses Stan.  The only thing that needs to be done
is to include the relevant libraries.  

The compiler command-line option to include a
header-only library is 
%
\begin{quote}
\code{-I {\slshape path-to-library}}.
\end{quote}
%
The path to the library must be such that any \code{\#include}
statements within the \Cpp source files be resolvable starting from
the path to the library.  

The header-only library code for \Stan itself is located in the
subdirectory \code{src/} of the top-level \Stan directory.  To allow
programs to use the \Stan library, the compiler needs to be given the
option 
%
\begin{quote}
\code{-I {\slshape stan}/src} 
\end{quote}
%
where \code{\slshape stan} is the
path to the top-level Stan directory.  If the compiler is called from the
top-level Stan directory, it suffices to use \code{-I src}, as in the
examples in the first chapter.

\Stan depends on two open-source libraries,
%
\begin{enumerate}
\item Boost general purpose \Cpp libraries, and 
\item Eigen matrix and linear algebra \Cpp libraries
\end{enumerate}
%
These are both distributed along with \Stan in the directory
\code{{\slshape stan}/lib/}, where again \code{\slshape stan} is the
top-level directory of the \Stan distribution.  Both libraries take
include paths starting under \code{lib/}.  For most uses of \Stan, it
is also necessary to include an explicit compiler option to include
Eigen and Boost,
%
\begin{quote}
\code{-I {\slshape stan}/lib}
\end{quote}
%
with \code{\slshape stan} being the location of the top-level \Stan
directory.  Thus calling \Stan typically requires all three of \Stan,
Boost, and Eigen to be included, which is accomplished with
%
\begin{quote}
\code{-I {\slshape stan}/lib -I {\slshape stan}/src}
\end{quote}
%
where \code{\slshape stan} is the path to the top-level \Stan directory.


\section{Compiler Optimization}

Stan was written with an optimizing compiler in mind.  For
that reason, it runs as much as an order of magnitude or more
faster with optimization turned on.  

For development, we recommend optimization level 0, whereas for
sampling, we recommend optimization level 3.  These are controlled
through the compiler option \code{-O} (capital letter `O').  To
generate efficient code, use
%
\begin{quote}
\code{-O3}
\end{quote}
%
where the first character is the capital letter `O'.
For faster compile time but less efficient code, use
%
\begin{quote}
\code{-O0}
\end{quote}
%
where the first character is the capital letter `O' and
the second character is the digit `0'.

\section{Executable Name}

If no name is provided for the executable, the default value of
\code{a.out} is used.  This executable will show up in the directory
from which the compiler was called.

To put the executable in a different location, the \code{-o {\slshape
    path-to-executable}} command may be used.  In an earlier example,
\code{-o bern} was used to write the executable to a file called
\code{bern}.  (In Windows, executables are suffixed with \code{.exe}.)



\chapter{Running a \Stan Program}\label{stan-cmd.chapter}

Once a \Stan program has been compiled (see \refchapter{stanc}), 









\chapter{Dump Data Format}\label{dump.chapter}

For representing structured data in files, \Stan uses the dump format
introduced in \SPLUS and used in \R and \JAGS (and in \BUGS, but with
a different ordering).   A dump file is structured as a sequence of
variable definitions.  Each variable is defined in terms of its
dimensionality and its values.   There are three kinds of variable
declarations, one for scalars, one for sequences, and one for general
arrays.

\section{Scalar Variables}

A simple scalar value can be thought of as having an empty list of
dimensions.  Its declaration in the dump format follows the \SPLUS
assignment syntax.  For example, the following would constitute a
valid dump file defining a single scalar variable \code{y} with value
17.2.
%
\begin{quote}
\begin{Verbatim}
y <- 
17.2
\end{Verbatim}
\end{quote}
%
A scalar value is just a zero-dimensional array value.

\section{Sequence Variables}

One-dimensional arrays may be specified directly using the \SPLUS
sequence notation.  The following example defines an integer-value and
a real-valued sequence.
%
\begin{quote}
\begin{Verbatim}
n <- c(1,2,3)
y <- c(2.0,3.0,9.7)
\end{Verbatim}
\end{quote}
%
It is possible to define an array without a declaration of
dimensionality because the reader just counts the number of entries to
determine the size of the array.

\section{Array Variables}

For more than one dimension, the dump format uses a dimensionality
specification.  For example,
%
\begin{quote}
\begin{verbatim}
y <- structure(c(1,2,3,4,5,6), .Dim = c(2,3))
\end{verbatim}
\end{quote}
%
This defines a $2 \times 3$ array.  Data is stored in column-major
order, meaning the values for \code{y} will be as follows.
%
\begin{quote}
\begin{Verbatim}
y[1,1] = 1     y[2,1] = 3     y[3,1] = 5    
y[2,1] = 2     y[2,2] = 4     y[3,2] = 6
\end{Verbatim}
\end{quote}
%
The \code{structure} keyword just wraps a sequence of values and a
dimensionality declaration, which is itself just a sequence of
non-negative integer values.  The product of the dimensions must equal
the length of the array.


\section{Integer- and Real-Valued Variables}

There is no declaration in a dump file that distinguishes integer
versus real values.  If a value in a dump file's definition of a
variable contains a decimal point, \Stan assumes that the values are
real.  If there are no decimal points in a variable's defined value, 
the value may be assigned to variables declared as integer or real
in \Stan.

The following dump file declares an integer value for \code{y}.
%
\begin{quote}
\begin{Verbatim} 
y <- 
2
\end{Verbatim}
\end{quote}
% 
This definition can be used for a \Stan variable \code{y} declared as
\code{real} or as \code{int}.  Assigning an integer value to a real
variable automatically promotes the integer value to a real value.

The following dump file provides a real value for \code{y}.
%
\begin{quote}
\begin{Verbatim}
y <-
2.0
\end{Verbatim}
\end{quote}
%
Even though this is a round value, the occurrence of the decimal
point in the value, \code{2.0}, causes \Stan to infer that \code{y} is
real valued.  This dump file may only be used for variables \code{y}
delcared as real in \Stan.


\section{Quoted Variable Names}

In order to support \JAGS data file, variables may be double quoted.
For instance, the following definition is legal in a dump file.
%
\begin{quote}
\begin{Verbatim}
"y" <-
c(1,2,3)
\end{Verbatim}
\end{quote}

\section{Line Breaks}

The line breaks in a dump file are required to be consistent with
the way \R reads in data.  Both of the following declarations are
legal.
%
\begin{quote}
\begin{Verbatim}
y <- 2
y <-
3
\end{Verbatim}
\end{quote}
%
Following its roots in \R, breaking before the assignment arrow is not
allowed.
%
\begin{quote}
\begin{Verbatim}
y
<- 2  # Syntax Error
\end{Verbatim}
\end{quote}

Lines may also be broken in the middle of sequences declared
using the \code{c(...)} notation., as well as between the comma
following a sequence definition and the dimensionality declaration.
For example, the following declaration of a $1 \times 2 \times 3$
array is valid.
%
\begin{quote}
\begin{Verbatim}
y <-
structure(c(1,2,3,
4,5,6,7,8,9,10,11,
12), .Dim = c(2,3,
4))
\end{Verbatim}
\end{quote}

\section{General R Sequence Syntax}

Sometimes, \R will use shorthand for its output. For example,
starting \R,

\begin{quote}
\begin{Verbatim}[fontshape=sl]
> R
\end{Verbatim}
\end{quote}
%
and then within \R, executing the following commands,

\begin{quote}
\begin{Verbatim}[fontshape=sl]
R> e <- matrix(c(1,2,3,4,5,6),nrow=2,ncol=3)
R> dump("e")
\end{Verbatim}
\end{quote}
%
leads to a \code{dumpdata.R} file being created with
the following contents.
%
\begin{quote}
\begin{Verbatim}
e <-
structure(c(1, 2, 3, 4, 5, 6), .Dim = 2:3)
\end{Verbatim}
\end{quote}
%
\R has used the fact that it allows a contiguous
sequence to be specified with its start and end point
using the notation \code{2:3}.  \Stan cannot currently
parse this format of input.  

Alternatively, \R is prone to include long-integer specifiers.
For instance, a $2 \times 2$ matrix is dumped as follows.
%
\begin{quote}
\begin{Verbatim}
f <-
structure(c(1, 2, 3, 4), .Dim = c(2L, 2L))
\end{Verbatim}
\end{quote}
%
Here the dimensions are defined to be \code{c(2L,~2L)}.  \Stan 
always treats these dimesions as \code{L}.







\part{Modeling Language Reference}

\chapter{Data Types}

\section{Basic Data Types}

There are two basic data types in \Stan, integers and reals,
declared as \code{int} and \code{real}.

\subsection{Integers}

Stan uses 64-bit (8-byte) integers for all of its integer
representations.  The maximum value that can be represented
as an integer is $2^{63}-1$; the minimum value is $-(2^{63})$.

When integers overflow, their values wrap.  Thus it is up to
the \Stan programmer to make sure the integer values in their programs
stay in range.  In particular, every intermediate expression must have
an integer value that is in range.

\subsection{Reals}

\Stan uses 64-bit (8-byte) floating point representations of real
numbers.  \Stan roughly%
%
\footnote{\Stan compiles integers to \code{long int} and reals to
  \code{double} types in \Cpp.  Precise details of rounding wil depend
  on the compiler and hardware architecture on which the code is run.}
%
follows the {\sc ieee} 754 standard for floating-point computation.
The range of a 64-bit number is roughly $\pm 2^{1022}$, which is
slightly larger than $\pm 10^{307}$.  It is a good idea to stay well
away from such extreme values in \Stan models as they are prone to
cause overflow.

There are three special real values used to represent (1) error
conditions, (2) positive infinity, and (3) negative infinity.  The
error value is referred to as ``not a number.''

\subsection{Promoting Integers to Reals}

\Stan automatically promotes integer values to real values if
necessary, but does not automatically demote real values to integers.
For examples, real values may only be assigned to real variables, but
integer values may be assigned to either integer variables or real
variables.  What happens interally is that the integer representation
is converted to a floating-point representation.  This operation is
not free and is thus best avoided if possible.


\section{Vector Data Type}

\Stan supports a basic real-valued vector data type.  Vectors are
taken to be column vectors unless specified as row vectors.  

There are two subtypes of the basic vector data for vectors of data
meeting constraints.

\subsection{Simplex}

\subsection{Positive Ordered}


\section{Array Data Types}

If {\tt T} is a data type, then {\tt T[N]} is the type of an ordered
sequence of {\tt N} objects of type {\tt T}.  

\Stan supports arrays of data of arbitrary dimensionality.
Conceptually, multi-dimensional arrays are 





\chapter{Modeling Language Reference}\label{model-ref.chapter}

\Stan's modeling language is more procedural than what users may be
familiar with from BUGS and JAGS, both of which are declarative.  For
instance, \Stan statements are executed in the order they are written
in model specifications and local variables may be reassigned as in a
procedural programming language.  Furthermore, variables must be
declared before they are use.  

Like most programming languages, \Stan is defined syntactically in
terms of expressions, which denote values, and statements, which
denote an action to be taken.

The purpose of \Stan's modeling language is to allow users to write
down a probability function up to a multiplicative normalizing
constant.  


\section{Expressions}

The top-level grammar for expressions is provided in
\reffigure{expression-grammar}.

\begin{figure}
\begin{center}
{%\footnotesize
\begin{Verbatim}
expression ::= literal
             | variable
             | expression infixOp expression
             | prefixOp expression
             | expression postfixOp
             | expression '[' expressions ']'
             | function '(' expressions ')'
             | '(' expression ')'
\end{Verbatim}
}
\end{center}
\mycaption{expression-grammar}
          {The top-level expression grammar for \Stan model specifications.}
\end{figure}

Valid \Stan expresions include simple numerical literals (e.g.,
\code{2}, \code{32.7}), identifiers representing variables (e.g.,
\code{theta}), 

The simplest form of expression is a literal denoting a
value, such as \code{32.7}.  Expressions may also consist of


\part{Built-In Functions} 

\chapter{Built-in Functions}

\Stan supports a wide range of built-in functions.  These functions
operate over and return values spanning the complete range of data
types.  

The following chapters in this part describe basic functions returning
integers and returning real values, array and matrix functions, and
probability functions and cumulative distributions.

\section{Function Signatures}

The documentation of the special functions begins with a function
signature.  This determines the type of arguments taken by the
function and the type of value returned.  

\subsection{Overloading}

\Stan relies on function overloading, which means there can be several
different functions with the same name.  Functions are determined
uniquely by the combination of their name and the type of their
arguments.

For example, \Stan includes two built-in functions named \code{max}, one that
operates on integers to return an integer, with signature
%
\begin{quote}
\begin{Verbatim}
int max(int m, int n)
\end{Verbatim}
\end{quote}
%
and one that operates over reals, with signature
%
\begin{quote}
\begin{Verbatim}
real max(real x, real y)
\end{Verbatim}
\end{quote}
%
The appropriate instance is called based on the arguments provided.

The two functions named \code{max} are distinguished by their argument
types.  It would not be legal to add a third function with signature
\code{real max(int,int)}, as it would lead to a conflict with the
existing function of name \code{max} and argument types
\code{(int,int)}, even though it has a different return type.


\section{Integer-to-Real Type Promotion}

Integer values may be passed to functions requiring real values.  This
is not unrelated to the fact that integer values may be assigned to
real variables.  In both cases, the underlying \Cpp code handles the
promotion of the underlying integer type to the underlying
floating-point type.

Consider again the two functions \code{max(int,int)} and
\code{max(real,real}), and suppose these are the only two functions
named \code{max} (they are the only two of that name built into
\Stan).  If \code{m} is an integer variable and \code{x} a real
variable, the call \code{max(m,x)} must be resolved as referring to
\code{max(double,double)}, because integers can be promoted to real
values, but not vice-versa.  

If \code{n} is another integer variable, a call to \code{max(m,n)}
could conceivably call either function named \code{max}.  The function
\code{max(int,int)} requires no promotions, whereas
\code{max(double,double)} requires two, so the first is invoked.  The
general rule is that the function call matches the function requiring
the fewest promotions.

Even this simple disambiguation scheme leads to possible ambiguity
(though \Stan's built-in functions avoid it).  If there were two functions,
\code{foo(int,real)} and \code{foo(real,int)} and if \code{m} and
\code{n} were integer variales, a call to \code{foo(m,n)} would be
ambiguous.  Either function could be matched with one promotion and
there is no better-matching function.  In such cases, the call will
not compile.  This could be resolved by assigning one variable to a
temporary, as in the following example.
%
\begin{quote}
\begin{Verbatim}
int m;  int n;  real x;  real y;  
x <- m;  y <- n;
# foo(m,n);  // illegal!
foo(x,n);    // calls foo(double,int)
foo(m,y);    // calls foo(int,double)
\end{Verbatim}
\end{quote}





\chapter{Integer-Valued Basic Functions}

This chapter describes \Stan's built-in function that operate on real
and integer arguments and return results of type integer.

\section{Absolute Function}

\begin{description}
%
\fitem{int}{abs}{int \farg{x}}{
The absolute value of \farg{x}}
%
\end{description}
%

\section{Bound Functions}
%
\begin{description}
\fitem{int}{min}{int \farg{x}, int \farg{y}}{
The minimum of \farg{x} and \farg{y}}
%
\fitem{int}{max}{int \farg{x}, int \farg{y}}{
The maximum of \farg{x} and \farg{y}}
%
\end{description}


\chapter{Real-Valued Basic Functions}

This chapter describes built-in functions that take zero or more real
or integer arguments and return real values.  Constants are
represented as functions with no arguments and must be called as such.

\section{Mathematical Constants}

%
\begin{description}
%
\fitem{real}{pi}{}{
  $\pi$, the ratio of a circle's circumference to its diameter}
%
\fitem{real}{e}{}{
 $e$, the base of the natural logarithm}
%
\fitem{real}{sqrt2}{}{
The square root of 2}
%
\fitem{real}{log2}{}{
The natural logarithm of 2}
%
\fitem{real}{log10}{}{
The natural logarithm of 10}
%
\end{description}

\section{Special Values}

\begin{description}
\fitem{real}{nan}{}{
Not-a-number, a special non-finite real value returned to signal an error}
%
\fitem{real}{infinity}{}{
 Positive infinity, a special non-finite real value larger than all
  finite numbers}
%
\fitem{real}{negative\_infinity}{}{ 
 Negative infinity, a special non-finite real value smaller than all
  finite numbers}
%
\fitem{real}{epsilon}{}{
The smallest positive real value representable}
%
\fitem{real}{negative\_epsilon}{}{
The largest negative real value representable}
%
\end{description}

\section{Logical Functions}

\begin{description}
%
\fitem{real}{if\_else}{int cond, real \farg{x}, real \farg{y}}{
\farg{x} if \farg{cond} is non-zero, and \farg{y} otherwise}
%
\fitem{real}{step}{real \farg{x}}{
0 if \farg{x} is negative and 1 otherwise}
%
\end{description}


\section{Absolute Functions}

\begin{description}
%
\fitem{real}{abs}{real \farg{x}}{
The absolute value of \farg{x}}
%
\fitem{real}{fabs}{real \farg{x}}{
The absolute value of \farg{x}}
%
\fitem{real}{fdim}{real \farg{x}, 
                 real \farg{y}}{
The positive difference between \farg{x} and \farg{y}, which is
\farg{x} - \farg{y} if \farg{x} is greater than \farg{y} and 0 otherwise}
%
\end{description}

\section{Bounds Functions}

\begin{description}
%
\fitem{real}{fmin}{real \farg{x}, real \farg{y}}{
The minimum of \farg{x} and \farg{y}}
%
\fitem{real}{fmax}{real \farg{x}, real \farg{y}}{
The maximum of \farg{x} and \farg{y}}
%
\end{description}

\section{Arithmetic Functions}
%
\begin{description}
\fitem{real}{fmod}{real \farg{x}, real \farg{y}}{
The real value remainder after dividing \farg{x} by \farg{y}}
%
\end{description}

\section{Rounding Functions}

\begin{description}
%
\fitem{real}{floor}{real \farg{x}}{
The floor of \farg{x}, which is the largest integer less
than or equal to \farg{x}, converted to a real value}
%
\fitem{real}{ceil}{real \farg{x}}{
The ceiling of \farg{x}, which is the smallest integer greater
than or equal to \farg{x}, converted to a real value}
%
\fitem{real}{round}{real \farg{x}}{
The nearest integer to \farg{x}, converted to a real value}
%
\fitem{real}{trunc}{real \farg{x}}{
The integer nearest to but no larger in magnitude than \farg{x},
converted to a double value}
%
\end{description}

\section{Power and Logarithm Functions}

\begin{description}
%
\fitem{real}{sqrt}{real \farg{x}}{
The square root of \farg{x}}
%
\fitem{real}{cbrt}{real \farg{x}}{
The cube root of \farg{x}}
%
\fitem{real}{square}{real \farg{x}}{
The square of \farg{x}}
%
\fitem{real}{exp}{real \farg{x}}{
The natural exponential of \farg{x}}
%
\fitem{real}{exp2}{real \farg{x}}{
The base-2 exponential of \farg{x}}
%
\fitem{real}{expm1}{real \farg{x}}{
The natural exponential of \farg{x} minus 1}
%
\fitem{real}{log}{real \farg{x}}{
The natural logarithm of \farg{x}}
%
\fitem{real}{log2}{real \farg{x}}{
The base-2 logarithm of \farg{x}}
%
\fitem{real}{log10}{real \farg{x}}{
The base-10 logarithm of \farg{x}}
%
\fitem{real}{pow}{real \farg{x}, real \farg{y}}{
\farg{x} raised to the power of \farg{y}}
%
\end{description}




\section{Link Functions}

\begin{description}
%
\fitem{real}{logit}{real \farg{x}}{
The log odds, or logit, function applied to \farg{x}}
%
\fitem{real}{inv\_logit}{real \farg{x}}{
The logistic sigmoid function applied to \farg{x}}
%
\fitem{real}{inv\_cloglog}{real \farg{x}}{
The inverse of the complement log-log function applied to \farg{x}}
%
\end{description}


\section{Trigonometric Functions}

\begin{description}
%
\fitem{real}{hypot}{real \farg{x}, real \farg{y}}{
The length of the hypoteneuse of a right triangle with sides of
length \farg{x} and \farg{y}}
%
\fitem{real}{cos}{real \farg{x}}{
The cosine of the angle \farg{x} (in radians)}
%
\fitem{real}{sin}{real \farg{x}}{
The sine of the angle \farg{x} (in radians)}
%
\fitem{real}{tan}{real \farg{x}}{
The tangent of the angle \farg{x} (in radians)}
%
\fitem{real}{acos}{real \farg{x}}{
The princpipal arc (inverse) cosine (in radians) of \farg{x}}
%
\fitem{real}{asin}{real \farg{x}}{
The principal arc (inverse) sine (in radians) of \farg{x}}
%
\fitem{real}{atan}{real \farg{x}}{
The principal arc (inverse) tangent (in radians) of \farg{x}}
%
\fitem{real}{atan2}{real \farg{x}, real \farg{y}}{
The principal arc (inverse) tangent (in radians) of \farg{x} divided
by \farg{y}}
%
\end{description}

\section{Hyperbolic Trigonometric Functions}

\begin{description}
%
\fitem{real}{cosh}{real \farg{x}}{
The hyperbolic cosine of \farg{x} (in radians)}
%
\fitem{real}{sinh}{real \farg{x}}{
The hyperbolic sine of \farg{x} (in radians)}
%
\fitem{real}{tanh}{real \farg{x}}{
The hyperbolic tangent of \farg{x} (in radians)}
%
\fitem{real}{acosh}{real \farg{x}}{
The inverse hyperbolic cosine (in radians) of \farg{x}}
%
\fitem{real}{asinh}{real \farg{x}}{
The inverse hyperbolic sine (in radians) of \farg{x}}
%
\fitem{real}{atanh}{real \farg{x}}{
The inverse hyperbolic tangent (in radians) of \farg{x}}
%
\end{description}

\section{Probability-Related Functions}

\begin{description}
%
\fitem{real}{erf}{real \farg{x}}{
The error function of \farg{x}}
%
\fitem{real}{erfc}{real \farg{x}}{
The complementary error function of \farg{x}}
%
\fitem{real}{Phi}{real \farg{x}}{
The cumulative normal density function of \farg{x}}
%
\fitem{real}{log\_loss}{int \farg{y}, real \farg{y\_hat}}{
The log loss of predicting probabilty \farg{y\_hat} for 
binary outcome \farg{y}}
%
\end{description}




\section{Combinatorial Functions}

\begin{description}
%
\fitem{real}{tgamma}{real \farg{x}}{
The gamma function applied to \farg{x}}
%
\fitem{real}{lgamma}{real \farg{x}}{
The natural log of the gamma function applied to \farg{x}}
%
\fitem{real}{lmgamma}{int \farg{n}, real \farg{x}}{
The natural logarithm of the multinomial gamma function with \farg{n}
dimensions applied to \farg{x}}
%
\fitem{real}{lbeta}{real \farg{x}, real \farg{y}}{
The natural log of the beta function applied to \farg{x}}
%
\fitem{real}{binomial\_coefficient\_log}{real \farg{x}, real \farg{y}}{
The natural logarithm of the binomial coefficient of \farg{x} choose
\farg{y}, generalized to real values via the gamma function}
%
\end{description}


\section{Composed Functions}

\begin{description}
%
\fitem{real}{fma}{real \farg{x}, 
               real \farg{y},
               real \farg{z}}{
\farg{z} plus the result of \farg{x} multiplied by \farg{y}}
%
\fitem{real}{multiply\_log}{real \farg{x}, real \farg{y}}{ 
The product of \farg{x} and the natural logarithm of \farg{y}}
%
\fitem{real}{log1p}{real \farg{x}}{
The natural logarithm of 1 plus \farg{x}}
%
\fitem{real}{log1m}{real \farg{x}}{
The natural logarithm of 1 minus \farg{x}}
%
\fitem{real}{log1p\_exp}{real \farg{x}}{ 
The natural logarithm of one plus the natural exponentiation of
\farg{x}}
%
\fitem{real}{log\_sum\_exp}{real \farg{x}, real \farg{y}}{ 
The natural logarithm of the sum of the natural exponentiation
of \farg{x} and the natural exponentiation of \farg{y}}
%
\end{description}




\chapter{Array Operations}

\chapter{Matrix Operations}

\chapter{Discrete Probability Functions}

\chapter{Continuous Probability Functions} 

\chapter{Cumulative Distributions}



\part{Advanced Topics}


\chapter{Variable Transforms}

To avoid having to deal with constraints while simulating the
Hamiltonian dynamics during sampling, every (multivariate) parameter
in a \Stan model is transformed to an unconstrained variable behind
the scenes by the model compiler.  The transform is based on any
constraints in the parameter's definition.  Constraints that may be
placed on variables include upper and lower bounds, positive ordered
vectors, simplex vectors, correlation matrices and covariance
matrices.  This chapter provides a definition of the transforms used
for each type of variable.

Once the model is compiled, it has support on all of
$\mathbb{R}^K$, where $K$ is the number of unconstrained parameters
needed to define the actual parameters defined in the model.

The details of section need not be understood in order to use
\Stan for well-behaved models.  Understanding the sampling behavior
of \Stan fully requires understanding these transforms.


\section{Changes of Variables}

The support of a random variable $X$ with density $p_X(x)$ is that
subset of values for which it has non-zero density,
%
\[
\mbox{support}(X) = \{ x | p_X(x) > 0 \}.
\]

If $f$ is a total function defined on the support of $X$, then $Y =
f(X)$ is a new random variable.  This section shows how to compute the
probability density function of $Y$ for well-behaved transforms $f$
and the rest of the chapter details the transforms used by \Stan.



\subsection{Univariate Changes of Variables}

Suppose $X$ is one dimensional and $f: \mbox{support}(X) \rightarrow
\mathbb{R}$ is a one-to-one, monotonic function with a differentiable
inverse $f^{-1}$.  Then the density of $Y$ is given by
%
\[
p_Y(y) = p_X(f^{-1}(y))  
         \,
         \left| \, \frac{d}{dy} f^{-1}(y)\, \right|.
\]


\subsection{Multivariate Changes of Variables}


Suppose $X$ is a $K$-dimensional random variable with probability
density function $p_X(x)$.  A new random variable $Y = f(X)$ may be
defined by transforming $X$ with a suitably well-behaved function $f$.
It suffices for what follows to note that if $f$ is one-to-one
and its inverse $f^{-1}$ has a well-defined Jacobian, then the
density of $Y$ is
%
\[
p_Y(y) = p_X(g(y)) \, \left| \, \det \, J_g(y) \, \right|,
\]
%
where $\det{}$ is the matrix determinant operation and $J_{f^{-1}}(y)$ is
the Jacobian of $f^{-1}$ evaluated at $y$.  The latter is defined by
\[
J_{f^{-1}}(y) = 
\left[
\begin{array}{ccc}\displaystyle
\frac{\partial y_1}{\partial x_1}
& \cdots
& \displaystyle \frac{\partial y_1}{\partial x_{K}}
\\[6pt]
\vdots & \vdots & \vdots
\\
\displaystyle\frac{\partial y_{K}}{\partial x_1}
& \cdots
& \displaystyle\frac{\partial y_{K}}{\partial x_{K}}
\end{array}
\right].
\]
%
If the Jacobian is a triangular matrix, the determinant reduces to the
product of the diagonal entries,
%
\[
\det \, J_{f^{-1}}(y)
= \prod_{k=1}^K \frac{\partial y_k}{\partial x_k}.
\]
%
Triangular matrices naturally arise in situations where the variables
are ordered, for instance by dimension, and each variable's
transformed value depends on the previous variable's transformed
values.  Diagonal matrices, a simple form of triangular matrix,
arise if each transformed variable only depends on a single raw
variable.

\section{Lower Bounds}

\Stan uses a logarithmic transform for lower and upper bounds.  If a
variable $X$ is declared to have lower bound $c$, it is transformed to
a new variable $Y$, where
%
\[
Y = \log(X - c),
\]
%
the inverse transform is
%
\[
f^{-1}(y) = \exp(y) + c,
\]
%
and the density of $Y$ is
%
\begin{eqnarray*}
p_Y(y) 
& = & p_X\!\left( \exp(y) + c \right) \cdot \left| \frac{d}{dy} \left(
    \exp(y) + c \right) \right|
\\[6pt]      
& = & p_X\!\left( \exp(y) + c \right) \cdot \exp(y).
\end{eqnarray*}


\section{Upper Bounds}

\Stan treats upper bounds in the same way as lower bounds, with a
logarithmic transform.  Only this time the result must be negated.  If
a variable $X$ is declared to have an upper bound $d$, it is
transformed to a new variable $Y$, where
%
\[
Y = \log(d - X)
\]
%
the inverse transform is
%
\[
f^{-1}(y) = d - \exp(y)
\]
%
and the density of $Y$ is
%
\begin{eqnarray*}
p_Y(y) 
& = & p_X \!\left( d - \exp(y) \right) \cdot \left| \frac{d}{dy} \left( d
    - \exp(y) \right) \right|
\\[6pt]      
& = &  p_X \!\left( d - \exp(y) \right) \cdot \exp(y).
\end{eqnarray*}


\section{Lower and Upper Bounds}

For lower and upper-bounded variables, \Stan uses the log-odds
transform.  It scales and translates from the basic log-odds function
defined for $u \in (0,1)$ by
%
\[
\mbox{logit}(u) = \log \frac{u}{1 - u}.
\]
% 
The inverse of the log odds function is the logistic sigmoid, defined 
for $v \in (-\infty,\infty)$ by
%
\[
\mbox{logit}^{-1}(v) = \frac{1}{1 + \exp(-v)}.
\]
% 
The derivative of the logistic sigmoid is
%
\[
\frac{d}{dy} \mbox{logit}^{-1}(y) 
= \mbox{logit}^{-1}(y) \cdot \left( 1 - \mbox{logit}^{-1}(y) \right).
\]
 
For variables constrained to be in the open interval $(a,b)$, \Stan
uses a scaled and translated log-odds transform.  If variable $X$ is
declared to have lower bound $a$ and upper bound $b$, then it is
transformed to a new variable $Y$, where
%
\[
Y = \mbox{logit} \left( \frac{X - a}{b - a} \right).
\]
%
The inverse of this transform is
%
\[
X = a + (b - a) \cdot \mbox{logit}^{-1}(Y)
\]
%
and the density of the transformed variable is
%
\begin{eqnarray*}
p_Y(y) 
& = & p_X \! \left( a + (b - a) \cdot \mbox{logit}^{-1}(y) \right)
    \left|  \frac{d}{dy} a + (b - a) \cdot \mbox{logit}^{-1}(y)
    \right| .
\\[8pt]
& = & p_X \! \left( a + (b - a) \cdot \mbox{logit}^{-1}(y) \right)
    \cdot (b - a)
    \cdot \mbox{logit}^{-1}(y)
    \cdot \left( 1 - \mbox{logit}^{-1}(y) \right).
\end{eqnarray*}
%
Despite its apparent complexity, $\mbox{logit}^{-1}(y)$, and hence
$\exp(-y)$, need only be evaluated once.


\section{Positive Ordered}

For some modeling tasks, a vector-valued random variable $X$ is
required with support on positive, ordered sequences.  One example is
the set of cut points in ordinal logistic regression.  

In constraint terms, a positive, ordered vector $x \in \mathbb{R}^K$
is a vector that satisfies
\[
0 < x_1
\]
%
and for $2 \leq k \leq K$,
\[
x_{k-1} < x_k
\]
%

\Stan's transform follows the constraint directly.  It maps a vector
$x \in \mathbb{R}^{K}$ to a vector $f(x) = y \in \mathbb{R}^K$ by setting
%
\[
y_1 = f_1(x) = \log x_1
\] 
%
and for $2 \leq k \leq K$,
\[
y_k = f_k(x) = \log \left( x_{k} - x_{k-1} \right).
\]

The inverse transform $x = f^{-1}(y)$ satisfies
%
\[
x_1 = \exp(y_1)
\]
%
and for $2 \leq k \leq K$, 
\[
x_k = x_{k-1} + \exp(y_k) = \sum_{k' =1}^{k} \exp(y_{k'}).
\]

The Jacobian of the inverse transform $f^{-1}$ is lower triangular,
with diagonal elements for $1 \leq k \leq K$ of
\[
J_{k,k} = \frac{\partial}{\partial y_k} f_k^{-1}(y) = \exp(y_k).
\]
%
Because of the triangularity and the positivivity of the $x_k$ and
their differences, the absolute determinant of the Jacobian is
%
\[
\left| \, \det \, J \, \right|
\ = \ 
\prod_{k=1}^K J_{k,k}
\ = \ 
\prod_{k=1}^K \exp(y_k).
\]


Putting this all together, if $p_X$ is the density of $X$, then the
transformed variable $Y$ has density $p_Y$ given by
%
\[
p_Y(y)
= p_X(f^{-1}(y)) 
\
\prod_{k=1}^K \exp(y_k).
\]


\section{Unit Simplex}

The parameter of the $K$-dimensional categorical distribution must lie
in the unit $K$-simplex.  Consequently, simplex-constrained variables show
up in multivariate discrete models of all kinds.  

The $K$-simplex is the set of points $x \in \mathbb{R}^K$ such that
for $1 \leq k \leq K$, 
\[ 
x_k > 0,
\] 
and
\[
\sum_{k=1}^K x_k = 1.
\]
%   
An alternative definition is to take the hull of the convex closure of
the vertices.  For instance, in 2-dimensions, the basis points are the
extreme values $(0,1)$, and $(1,0)$ and the unit 2-simplex is interval
with these as the end points.  In 3-dimensions, the basis is
$(0,0,1)$, $(0,1,0)$ and $(1,0,0)$ and the unit 3-simplex is the
triangle with these vertices.  As these examples illustrate, the
simplex always picks out a subspace of $K-1$ dimensions from
$\mathbb{R}^K$.

A point $x$ in the $K$-simplex is fully determined by its first $K-1$
dimensions, because rearranging terms in the constraint yields
%
\[
x_K = 1 - \sum_{k=1}^{K-1} x_k.
\]
%

Stan employs a transform whose inverse may be understood using a
stick-breaking metaphor.  A simplex is determined by taking a stick of
unit length, breaking a piece off, the length of which is $x_1$.  Then
$x_2$ is determined by breaking a piece from what's left.  A total of
$K-1$ pieces are broken off, determining $x_1,\ldots,x_{K-1}$.  To
complete the metaphor, the length of the remaining piece after $K-1$
pieces are broken off determines $x_K$.

The simplex transform $f$ is most easily understood in terms of its
inverse $x = f^{-1}(y)$, which maps a point in $y \in
\mathbb{R}^{K-1}$ to a point $x$ in the $K$-simplex.  An intermediate
vector $z \in \mathbb{R}^{K-1}$, whose coordinates $z_k$ represent 
the proportion of the stick broken off in step $k$, is defined
elementwise for $1 \leq k < K$ by
%
\[
z_k = \mbox{logit}^{-1} \left( y_k 
                             - \mbox{logit} \left( \frac{1}{K - k + 1}
                                            \right)
                       \right).
\]
%
The logit term in the above definition adjusts the transform so that a
zero vector $y$ is mapped to $(1/K,\ldots,1/K)$.  For instance, if
$y_1 = 0$, then $z_1 = 1/K$; if $y_2 = 0$, then $z_2 = 1/(K-1)$; and
if $z_{K-1} = 0$, then $z_{K-1} = 1/2$.  This ensures that random
initializations for categorical distribution parameters are
initialized around a parameter value when $y = 0$ representing the
uniform distribution.

The break proportions $z$ are applied to determine the stick sizes and
resulting value of $x_k$ for $1 \leq k < K$ by
%
\[
x_k = 
\left( 1 - \sum_{k'=1}^{k-1} x_{k'} \right) z_k.
\]
%
The summation term represents the length of stick left at stage $k$.
This is multiplied by the break proportion $z_k$ to yield $x_k$.
Because $x$ lines in a $K$-simplex, $x_K$ is determined from
$x_1,\ldots,x_{K-1}$.

The Jacobian $J$ of the inverse transform $f^{-1}$ is
lower-triangular, with diagonal entries
\[
J_{k,k}
=
\frac{\partial x_k}{\partial y_k}
=
\frac{\partial x_k}{\partial z_k} \,
\frac{\partial z_k}{\partial y_k},
\]
%
where
\[
\frac{\partial z_k}{\partial y_K} 
= \frac{\partial}{\partial z_k} 
   \mbox{logit}^{-1} \left(
                       y_k - \mbox{logit} \left( \frac{1}{K-k+1}
                                          \right)
                    \right)
= z_k (1 - z_k),
\]
%
and
%
\[
\frac{\partial x_k}{\partial z_k}
=
\left( 
  1 - \sum_{k' = 1}^{k-1} x_{k'}
   \right)
.
\]
%
Note that the definition is recursive, definining $x_k$ in terms of
$x_{1},\ldots,x_{k-1}$.

Because the Jacobian $J$ of $f^{-1}$ is lower triangular and positve, its
absolute determinant reduces to
%
\[
\left| \, \det J \, \right|
\ = \
\prod_{k=1}^{K-1} J_{k,k}
\ = \
\prod_{k=1}^{K-1} 
z_k
\, 
(1 - z_k)
\
\left(
1 - \sum_{k'=1}^{k-1} x_{k'}
\right)
.
\]
%
Thus the transformed variable $Y = f(X)$ has a density given by
%
\[
p_Y(y) 
= p_X(f^{-1}(y))
\,
\prod_{k=1}^{K-1} 
z_k
\, 
(1 - z_k)
\
\left(
1 - \sum_{k'=1}^{k-1} x_{k'}
\right)
.
\]
%
This formula looks more complicated than it is.  It only involves a
single exponential function evaluation involved (in the logistic
sigmoid applied to $y_k$ to produce $z_k$);  everything else is just
basic arithmetic and keeping track of the remaing stick length.

The transform $Y = f(X)$ can be derived by reversing the stages of the
inverse transform.  Working backwards, given the break proportions
$z$, $y$ is defined elementwise by
%
\[
y_k 
= \mbox{logit}(z_k)
+ \mbox{logit}\left(
   \frac{1}{K-k+1}
   \right)
.
\]
%
The break proportions $z_k$ are defined to be the ratio of $x_k$ to
the length of stick left after the first $k-1$ pieces have been broken
off, 
%
\[
z_k 
= \frac{x_k}
       {1 - \sum_{k' = 1}^{k-1} x_k}
.
\]

\section{Correlation Matrices}

A correlation matirx is a symmetric, positive-definite matrix with a
unit diagonal.  To deal with this rather complicated constraint, \Stan
implements the transform of \cite{LewandowskiKurowickaJoe:2009},
henceforth the \LKJ-transform.  The number of free parameters required
to specify a $K \times K$ correlation matrix is $K \choose 2$.

\subsection{Correlation Matrix Inverse Transform}

It is easiest to specify this transform in reverse, going from its $K
\choose 2$ parameter basis to a correlation matrix.  The basis will
actually be broken down into two steps.  To start, suppose $y$
consists of $K \choose 2$ unconstrained values.  Next, define a $K
\times K$ matrix $z$ by first transforming $y$ using the $\tanh$ function.
%
\[
z_{i,j}
= \tanh y_{j(j - 1)/2 + i}
\]
%
where, $\tanh : \mathbb{R} \rightarrow (-1,1)$ by
%
\[
\tanh u = \frac{\exp(2x) - 1}{\exp(2x) + 1}.
\]
%
For example, in the $4 \times 4$ case, there are ${4 \choose 2}$
non-zero values arranged as
%
\[
z 
=
\left[
\begin{array}{cccc}
0 & \tanh y_1 & \tanh y_2 & \tanh y_3
\\
0 & 0 & \tanh y_4 & \tanh y_5
\\
0 & 0 & 0 & \tanh y_6
\\
0 & 0 & 0 & 0
\end{array}
\right]
.
\]
%
This mapping is one to one, resulting in $z_{i,j} \in (-1,1)$.
Lewandowski et al.\ show how to map the matrix $z$ to a correlation
matrix $x$.  The entry $z_{i,j}$ for $i < j$ is interpreted as the
canonical partial correlation (\CPC) between $i$ and $j$, which is
the correlation between $i$'s residuals and $j$'s residuals when both
$i$ and $j$ are regressed on all variales before. In the case of
$i=1$, there are no earlier variables, so $z_{i,j}$ is just the plain
correlation between $i$ and $j$.

The \LKJ transform can be formulated in terms of a Cholesky factor $w$
of the final correlation matrix, defined for $1 \leq i,j \leq K$ by
%
\[
w_{i,j} = 
\left\{
\begin{array}{cl}
%
0 & \mbox{if } i > j
\\[4pt]
1 & \mbox{if } 1 = i = j
\\[12pt]
\prod_{m=1}^{i - 1} \left( 1 - z_{m,j}^2 \right)^{1/2}
& \mbox{if } 1 < i = j
\\[12pt]
z_{i,j} & \mbox{if } 1 = i < j
\\[12pt]
z_{i,j} \, \prod_{m=1}^{i-1} \left( 1 - z_{m,j}^2 \right)^{1/2}
& \mbox{ if } 1 < i < j.
%
\end{array}
\right.
\]
%
This does not require as much computation per matrix entry as it may appear; 
calculating the rows in terms of earlier rows yields the more manageable
%
\[
w_{i,j} = 
\left\{
\begin{array}{cl}
%
0 & \mbox{if } i > j
\\[4pt]
1 & \mbox{if } 1 = i = j
\\[8pt]
z_{i,j} & \mbox{if } 1 = i < j
\\[8pt]
w_{i-1,j} \left( 1 - z_{i-1,j}^2 \right)^{1/2}
& \mbox{ if } 1 < i \leq j.
%
\end{array}
\right.
\]

Given the Cholesky factor, the final correlation matrix is
\[
x = w w^{\top}.
\]

Lewandowski et al.\ show that the determinant of the correlation
matrix can be defined in terms of the Cholesky factor $w$ as
%
\[
\mbox{det} \, x = \prod_{i=1}^{K-1} \ \prod_{j=i+1}^K \ (1 - w_{i,j}^2)
 = \prod_{1 \leq i < j \leq K} (1 - w_{i,j}^2).
\]


\subsection{Correlation Transform}

The final stage of the transform reverses the tanh transform, defining
\[
y_{j(j-1)/2 + i} = \tanh^{-1} z_{i,j}
\]
%
for $i < j$, where the inverse of tanh is given by
\[
\tanh^{-1} v = \frac{1}{2} \log \left( \frac{1 + v}{1 - v} \right).
\]






\section{Covariance Matrices}

Covariance matrices are just scaled correlation matrices.  This
requires an additional $K$ positive scaling parameters, for a total
requirement of $K + {K \choose 2}$ parameters to specify a covariance
matrix.  

\subsection{Covariance Matrix Inverse Transform}

Suppose $y$ is a $K \choose 2$-dimensional array specifying the $K
\times K$ correlation matrix $x$ as specified by the correlation
matrix inverse transform described in the previous section.  

Let $y'$ be a $K$-dimensional vector of unconstrained scaling
parameters.  An exponential transform converts these to positive
values component-wise, by
%
\[
u = \exp(y').
\]
%
The covariance matrix is the scaled version of the correlation matrix
$x$,
\[
v \ = \ \mbox{diag}(u) \ x \ \mbox{diag}(u)
  \ = \ \left(\mbox{diag}(u) \, z\right) \left(\mbox{diag}(u) \, z)\right)^{\top},
\]
%
where $\mbox{diag}(u)$ is the diagonal matrix with diagonal $u$.

\chapter{The C++ Model Class}

The generated \Cpp class extends a built-in \Stan abstract
base class for probability models.  Instances of the class are
constructed from a specified data vector $y$.  The data vector $y$
determines the dimensionality $K$ of the parameter vector $\theta$,
which in general may depend on size constants in $y$.  The class
implements a method that takes a parameter $K$-vector $\theta$ as
argument and returns the (unnormalized) total log probabilty,
\[
\theta 
\mapsto 
\log p(y,\theta) 
\]
The second method returns the gradient of the (unnormalized) total
probability as a function of a parameter $K$-vector $\theta$,
\[
\theta
\mapsto
\nabla_{\theta} \log p(y,\theta)
= ( \frac{\partial}{\partial\theta_1} \log p(y,\theta),
  \ldots, 
  \frac{\partial}{\partial\theta_K} \log p(y,\theta) ),
\]

The class computes gradients using accurate and efficient reverse-mode
algorithmic differentiaton.  The cost of computing the gradient is
a small multiple of the cost of computing the log probability.  The
cost inovlves a bounded amount of extra bookkeeping for each 
subexpression involved in computing the log probability.  Unlike
in the calculation of finite differences, the extra bookkeeping is
not dependent on the dimensionality of the parameter vector.


\chapter{Optimizing \Stan Code}\label{optimization.chapter}


\appendix

\part*{Appendices}
\addcontentsline{toc}{part}{Appendices}


\chapter{Installation}\label{install.appendix}

\chapter*{References}
\addcontentsline{toc}{chapter}{References}

\end{document}