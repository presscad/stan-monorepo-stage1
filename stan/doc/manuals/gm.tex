\documentclass[10pt]{report}

%\usepackage{times}
\usepackage[%romanfamily=times,
            scale=0.875,
            stdmathitalics=true,
            stdmathdigits=true]{lucimatx}
\usepackage{xspace}
\usepackage{fancyvrb}
\usepackage{titlesec}

\titleformat{\chapter}[hang]{\bfseries\huge}{\thechapter.}{1.5pc}{}{}
%\titlespacing{\chapter}{0pt}{-12pt}{18pt}{}
%
\titleformat{\section}{\bfseries\large}{\thesection.}{1em}{}
%\titlespacing{\section}{0pt}{12pt}{6pt}
%
\titleformat{\subsection}{\it}{}{0em}{}
%\titlespacing{\subsection}{-1em}{12pt}{6pt}

\newcommand{\Stan}{Stan\xspace}
\newcommand{\stanc}{{\ttfamily stanc}\xspace}
\newcommand*{\Cpp}{C\raise.2ex\hbox{\footnotesize ++}\xspace} %\ensuremath{++}
\newcommand{\clang}{{\ttfamily clang\raise.2ex\hbox{\footnotesize ++}}\xspace} 
\newcommand{\gpp}{{\ttfamily g\raise.2ex\hbox{\footnotesize ++}}\xspace} 

\newcommand{\acronym}[1]{{\sc #1}\xspace}

\newcommand{\R}{\acronym{r}}
\newcommand{\SPLUS}{\acronym{s}}
\newcommand{\BUGS}{\acronym{bugs}}
\newcommand{\JAGS}{\acronym{jags}}
\newcommand{\MCMC}{\acronym{mcmc}}
\newcommand{\HMC}{\acronym{hmc}}
\newcommand{\NUTS}{\acronym{nuts}}
\newcommand{\MSVC}{\acronym{msvc}}


\newcommand{\code}[1]{{\tt #1}}
\newcommand{\mycaption}[2]{\caption{{\it #2}\label{#1.figure}}}

\newcommand{\refappendix}[1]{Appendix~\ref{#1.appendix}}
\newcommand{\refchapter}[1]{Chapter~\ref{#1.chapter}}
\newcommand{\reffigure}[1]{Figure~\ref{#1.figure}}

\usepackage{url}

\newcommand\thicktilde

\newcommand{\samp}{{\lower.78ex\hbox{\texttt{\char`\~}}}}%
%{{\lower1.1ex\hbox{\Large\texttt{\char`\~}}}}

\title{\Huge\bf \Stan's Modeling Language}
\author{\Stan's Development Team}
\date{Version 1.0} % \footnotesize \today}

\begin{document}

\maketitle

\begin{abstract}
  \Stan is a general-purpose \Cpp probabilistic modeling framework
  designed to support full Bayesian inference.  This document
  describes \Stan's modeling language for specifying the joint
  probability of observed data and unobserved parameters.  \Stan's
  modeling language is based on \BUGS and \JAGS, and like them, allows
  users to develop, fit, and evaluate Bayesian models without knowing
  \Cpp.  \Stan's compiler parses model specifications and generates
  \Cpp code.

  \Stan performs inference by Markov chain Monte Carlo (\MCMC)
  simulation, which in this case, samples parameter values from the
  posterior distribution of parameters given the observed data.  \Stan
  employs the no-U-turn sampler (\NUTS), an adaptive form of
  Hamiltonian Monte Carlo (\HMC) that alleviates the need for user
  tuning of \HMC's rather sensitive parameters.  By making effective
  use of the gradient of the log posterior, \HMC converges and
  explores the parameter distribution faster than Gibbs sampling or
  random-walk Metropolis-Hastings.  The improved sampling algorithm
  and tighter compiled coding allows \Stan to operate on data of
  larger scales and models of more complex structure than \BUGS or
  \JAGS.
\end{abstract}

\chapter{Introduction}

This document is a reference manual and getting started guide for
using \Stan's probabilistic modeling language.  After describing the
overall system in this introduction and providing a hands-on
quick-start guide in the following chapter, the remainder of the
document is devoted to fully documenting the behavior of
Stan's modeling language.

\Stan's modeling language and its execution behavior are similar to
that of its progenitors, \BUGS and \JAGS.  It differs in many
particulars of the modeling language, which is more like an
impertative programming language than the declarative specifications
of \BUGS and \JAGS.

\section{\Stan's Modeling Language}

\noindent
\Stan's modeling language allows users to code a Bayesian model
specifying a joint probability function
\[
p(y,\theta),
\]
where 
\begin{itemize}
\item
  $y$ is a vector of known values, such as 
  constants, hyperparameters, and modeled data, and
\item
 $\theta$ is a vector of unknown values, such as estimated parameters,
  missing data, and simulated values.
\end{itemize}
%
To simplify terminology, $y$ will be called the data vector and
$\theta$ the parameter vector.  The probability function $p(y,\theta)$
need only be specified up to a multiplicative constant with respect to
any fixed data vector $y$.  This ensures proportionality of the
posterior to the specified joint probability,
\[
p(\theta|y) \propto p(y,\theta) = p(\theta|y) \, p(y).
\]

Stan's language is more imperative than its declarative 
predecessors, \BUGS and \JAGS.  Statements are executed in the order 
they are specified and variables and expressions are strongly typed 
and declared as data or parameters in the model rather than by a 
calling function.  Stan also supports a broader range of arithmetic, 
matrix, and linear algebra operations than \BUGS or \JAGS.  Users may 
manipulate log probability functions directly and are not required 
to use proper priors.

\section{\Stan's Compiler}

\Stan's compiler, \stanc, reads a user program in \Stan's modeling
language and generates a \Cpp class implementing the model specified
by that program.  \Stan automatically applies a multivariate transform
(and its Jacobian determinant) to free any constrained parameters,
such as deviations (constrained to be positive), simplexes (a vector
constrained to be positive and to sum to 1), and covariance matrices
(positive definiteness).  The result is an unconstrained sampling (or
optimization) problem from the perspective of the sampler.  From the
user's perspective, this transform happens behind the scenes, driven
by the types declared for each of the parameters.

The generated \Cpp class can be plugged into Stan's continuous and
discrete samplers to read the data $y$ and then draw a sequence of
sample parameter vectors $\theta^{(m)}$ according to the posterior,
\[
p(\theta|y) = \frac{p(y,\theta)}{p(y)} \propto p(y,\theta).
\]
The resulting samples may be used for full Bayesian inference, much of
which can be carried out within Stan.

\section{\Stan's Samplers}

For continuous variables, Stan uses Hamiltonian Monte Carlo (\HMC)
sampling. \HMC is a Markov chain Monte Carlo (\MCMC) method based on
simulating the Hamiltonian dynamics of a fictional physical system in
which the parameter vector $\theta$ represents the position of a
particle in $K$-dimensional space and potential energy is defined to
be the negative (unnormalized) log probability.  Each sample in the
Markov chain is generated by starting at the last sample, applying a
random momentum to determine initial kinetic energy, then simulating
the path of the particle in the field.  Standard \HMC runs the
simulation for a fixed number of discrete steps of a fixed step size
and uses a Metropolis adjustment to ensure detailed balance of the
resulting Markovian system.  This adjustment treats the momentum term
of the Hamiltonian as an auxiliary variable, and the only reason for
rejecting a sample will be discretization error in computing the
Hamiltonian.

\HMC treats the position of a particle, 
log probability as a negative potential
energy function, then samples by adding random kinetic energy and
simulating the 

In addition to basic \HMC, Stan implements an adaptive
version of \HMC, the No-U-Turn Sampler (\NUTS).  \NUTS automatically
tunes step sizes and a diagonal mass matrix during warmup and then
adapts the number of leapfrog integration steps during sampling.
Stan is expressive enough to allow most discrete variables to be
marginalized out.  For the remaining discrete parameters, Stan uses
Gibbs sampling if there are only a few outcomes and adaptive slice
sampling otherwise.


\chapter{Getting Started}

This chapter is designed to help users get acquainted with the overall
design of the \Stan language and calling \Stan from the command line.
For installation information, see \refappendix{install}.
Later chapters are devoted to expanding on the material in this
chapter with full reference documentation.


\section{A Minimal Program}

Stan is distributed with several working models.  The simplest of
these is found in the following location relative to the top-level
distribution.
%
\begin{quote}
\begin{Verbatim}
src/models/basic_distributions/normal.stan
\end{Verbatim}
\end{quote}
%
The contents of this file are as follows.
%
\begin{quote}
\begin{Verbatim}
parameters {
  real y;
}
model {
  y ~ normal(0,1);
}
\end{Verbatim}
\end{quote}
%
The model's single parameter \code{y} is declared to take real values.
The probability model specifies that \code{y} has a normal
distribution with location 0 and scale 1.  Basically, this model will
sample a single unit normal variate.  

\section{Whitespace and Semicolons}

In Stan, every variable declaration and statement must be terminated
by a semicolon (\code{;}).  This is the convention followed by
programming languages such as \Cpp.  It is not the convention followed
by the statistical languages \R, \BUGS, or \JAGS.  

The reason for the \Cpp convention is to ensure that differences in
whitespace are not meaningful.  In \R, \BUGS, and \JAGS, the following
is a complete, legal statement.
%
\begin{quote}
\begin{Verbatim}
a <- b +
     c
\end{Verbatim}
\end{quote}
%
In contrast, the usual way of typesetting mathematics and laying out
code in programming languages, with the operator continuing the
expression beginning a new line, is invalid.
%
\begin{quote}
\begin{Verbatim}
a <- b
     + c
\end{Verbatim}
\end{quote}
%
The only difference is in the kind of whitespace between \code{b} and
\code{+} and between \code{+} and \code{c}.  In \Stan, there is no
whitespace-dependent behavior.  Neither of these is a complete
statement, whereas either one terminated with a semicolon is.  The
second form is recommended for \Cpp and \Stan.


\section{Compiling  with {\tt\bfseries stanc}}

Starting at Stan's home directory, written here as {\tt \$stan},
the model may be compiled by the Stan compiler, \stanc, into \Cpp code
as follows.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cd $stan
> stanc src/models/basic_distributions/normal.stan
\end{Verbatim}
%
\begin{Verbatim}
Model name=anon_model
Input file=src/models/basic_distributions/normal.stan
Output file=anon_model.cpp
\end{Verbatim}
\end{quote}
%
The output indicates the name of the model, here the default value
\code{anon\_model}, the input file from which the Stan program is
read, here \code{normal.stan}, and the output file to which the
generated \Cpp code is written, here \code{anon\_model.cpp}.  See
\refchapter{stanc} for more documentation on the \stanc compiler.

\section{Compiling the Generated Code}

The file generated by \stanc must next be compiled with a \Cpp
compiler by linking to \Stan's source and library directories using
the {\tt -I} option of the compiler.  The following example 
uses the \clang compiler for \Cpp.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> clang++ -I src -I lib anon_model.cpp 
\end{Verbatim}
\end{quote}
%
This command invokes the \clang compiler for \Cpp to create a
platform-specific executable in the default location, which is {\tt
  a.out} by convention.  If all goes well, as above, there is no
output to the console.  More information about compiling the \Cpp code
generated by Stan may be found in \refchapter{compiling-cpp}.
Installation information for \Cpp compilers may be found in
\refappendix{install}.

\section{Running the Sampler}

The executable resulting from compiling the generated \Cpp may be run
as follows.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./a.out
\end{Verbatim}
%
\begin{Verbatim}
STAN SAMPLING COMMAND
data = 
init = random initialization
samples = samples.csv
append_samples = 0
seed = 1331941513 (randomly generated)
chain_id=1 (default)
iter = 2000
warmup = 1000
thin = 1
leapfrog_steps = -1
max_treedepth = 10
epsilon = -1
epsilon_pm = 0
epsilon_adapt_off = 0
delta = 0.5
gamma = 0.05

Iteration: 2000 / 2000 [100%]  (Sampling)
\end{Verbatim}
\end{quote}
%
The program indicates to the standard output that the samples are
written to \code{samples.csv}.  The first few lines of this file
are comments about aspects of the run.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> cat samples.csv
\end{Verbatim}
\begin{Verbatim}
# Samples Generated by Stan
#
# stan_version_major=alpha
# stan_version_minor=0
# data=
# init=random initialization
# append_samples=0
# seed=1331941796
# chain_id=1
# iter=2000
# warmup=1000
# thin=1
# leapfrog_steps=-1
# max_treedepth=10
# epsilon=-1
# epsilon_pm=0
# delta=0.5
# gamma=0.05
...
\end{Verbatim}
\end{quote}
%
The ellipses notation, {\tt ...}, indicates that the output continues
beyond what's shown.  Here, what follows is the data in standard
comma-separate value ({\sc csv}) notation.
%
\begin{quote}
\begin{Verbatim}
...
lp__,treedepth__,y
-0.0126699,1,0.159185
-0.222796,1,-0.667527
-0.222796,1,-0.667527
-0.404457,1,-0.899397
...
\end{Verbatim}
\end{quote}
%
The first line consists of a header indicating the names of the
variables on the lines to follow, and each following line indicates a
single sampled value of the parameters.  The first column is reserved
for the (unnormalized) log probability (density) of the parameters,
with name {\tt lp\_\_} (the underscores are to prevent name conflicts
with user-defined model parameters).  The next values are for
reporting the behavior of the sampler.  In this case, the \NUTS
sampler was used, so there is a report of the depth of tree it
explored, with variable name {\tt treedepth\_\_}.  The remaining
values are parameters.  Here, the model has only one parameter, {\tt
  y}.  The first sampled value for {\tt y} is 0.159185, the second is
-0.667527, and so on.  

Note that the second sampled value is repeated.  This is not a bug.
Rather, it is the behavior to expect from a sampler using a Metropolis
acceptance step for proposals, as Stan's samplers \HMC and \NUTS do.

\section{Data}

\Stan allows data to be specified in programs, used in models, and
read into compiled \Stan programs. This section provides an example of
coding and running a \Stan program with data stored in a file in the
\SPLUS/\R dump format.

The Stan program in 
\begin{quote}
\begin{Verbatim}
src/models/basic_estimators/bernoulli.stan
\end{Verbatim}
\end{quote}
can be used to estimate a Bernoulli parameter \code{theta} from
\code{N} binary observations.  The file contains the following code.
%
\begin{quote}
\begin{Verbatim}
data {
  int(0,) N;
  int(0,1) y[N];
}
parameters {
  real(0,1) theta;
}
model {
  theta ~ beta(1,1);
  for (n in 1:N)
    y[n] ~ bernoulli(theta);
}
\end{Verbatim}
\end{quote}
%
This program declares two data variables in its \code{data} block.
The first data variable, \code{N}, is an integer encoding the number
of observations.  The declaration \code{int(0,)} indicates that
\code{N} must take on non-negative values.  The second data variable,
\code{y}, is declared as \code{y[N]}, specifying that it is an array
of \code{N} values.  Each of these values has the declared type,
\code{int(0,1)}, an integer between 0 and 1 inclusive, i.e., a binary
value.  The \code{N} individual binary values in the array \code{y}
are accessed using standard array notation, indexing from 1, as \code{y[1]},
\code{y[2]}, ..., \code{y[N]}.

The \code{parametes} block declares a single parameter, \code{theta}.
Its type is given as \code{real(0,1)}, meaning it takes on continuous
values between 0 and 1 inclusive.  The constraint is necessary in
order to ensure that \code{theta} takes on a legal value as the
success parameter in the Bernoulli distribution in which it is used in
the \code{model} block of the program.

The \code{model} block consists of a for-loop for the data.   The loop is
specified so that the body is executed for values of \code{n} between
\code{1} and \code{N} inclusive.  The body here is a sampling
statement specifying that the variable \code{y[n]} is modeled as
having a Bernoulli distribution with parameter \code{theta}.  

A sample data file for this program can be found in the file
\code{bernoulli.Rdata} in the same directory.  This data file has
the following contents.
%
\begin{quote}
\begin{Verbatim}
N <- 10
y <- c(0,1,0,0,0,0,0,0,0,1)
\end{Verbatim}
\end{quote}
%
A data file must contain appropriate values for all of the data
variables declared in the \Stan program's \code{data} block.  Here there
is a non-negative integer value for \code{N} and an array of length
\code{N} (i.e., 10) integer values between 0 and 1 inclusive.  The
array is coded using the \SPLUS sequence notation \code{c(...)}.
The dump format supported by \Stan is documented in \refchapter{dump}.

The program is compiled by \stanc and the \Cpp compiler in the same
way.  This time, the output model gets an explicitly specified name.
%
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> stanc --name=bern src/models/basic_estimators/bernoulli.stan 
\end{Verbatim}
\begin{Verbatim}
Model name=bern
Input file=src/models/basic_estimators/bernoulli.stan
Output file=bern.cpp
\end{Verbatim}
\end{quote}
%
As before, the \Cpp compiler needs to be given the name of
generated file.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> clang++ -O3 -I src -I lib -o bern bern.cpp
\end{Verbatim}
\end{quote}
%
There are two new compiler options here.  The option \code{-O3} sets
optimization to level 3, which generates much faster executable
code at the expense of slower compilation.  The name of the
executable is also specified, using the option \code{-o~bern}.  Now
the code may be executed by calling its executable with the data file
specified. 
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./bern --data=src/models/basic_estimators/bernoulli.Rdata
\end{Verbatim}
\end{quote}



\section{Proper and Improper Priors}

The model in the previous section does not contain a sampling
statement for \code{theta}.  The default behavior is to give
\code{theta} a uniform prior.  In this case, a uniform prior is proper
because \code{theta} is bounded to a finite interval.  Improper priors
are also allowed in \Stan programs; they arise from unconstrained
parameters without sampling statements.  The uniform prior could have
also been added explicitly by adding the following statement to the
\code{model} block of the program.
%
\begin{quote}
\begin{Verbatim} 
theta ~ uniform(0,1);
\end{Verbatim}
\end{quote}
% 
A third way to specify that \code{theta} has a uniform distribution
between 0 and 1 is with the beta distribution.
%
\begin{quote}
\begin{Verbatim}
theta ~ beta(1,1);
\end{Verbatim}
\end{quote}
%
The beta distribution is conjugate to the Bernoulli, but \Stan (at
least as of yet) does not make use of this information.  On the other hand,
these three approaches, no prior, uniform prior, and beta prior,
are equally efficient in \Stan's sampler, because their uniformity
can be determined at compile time and thus computations related to
them eliminated.  There is further discussion of \Stan optimization
in \refchapter{optimization}


\chapter{Compiling Stan Programs to C++}\label{stanc.chapter}

Preparing a \Stan program to be run involves two compilation steps,
%
\begin{enumerate}
\item compiling the \Stan program to \Cpp, and
\item compiling the resulting \Cpp to an executable.
\end{enumerate}
%
This chapter discusses the first step; the second step is discussed in
\refchapter{compiling-cpp}.

\section{The \stanc Compiler}

The \stanc compiler converts \Stan programs to \Cpp programs.  

The first thing it does is parse the \Stan program.  If the parser is
successful, it then generates \Cpp code.  If the parser fails, it will
provide an error message indicating where and why the error occurred.

The following example illustrates a fully qualified call to \stanc.
%
\begin{verbatim}
> stanc --name=binary_normal --o=binorm.cpp binormal.stan 
\end{verbatim}
%
This call specifies the name of the model, here {\tt binary\_normal}.
This will determine the name of the class implementing the model in
the \Cpp code.  The \Cpp code implementing the class is written to
\code{binorm.cpp}.  The final argument, \code{binormal.stan}, is
the file from which to read the \Stan program.


\section{Command-Line Options}

\begin{description}
%
\item[\tt --help] 
\mbox{ } \\ 
Displays the manual page for \stanc.  If this option is selected,
nothing else is done.
%
\item[\tt --version]
\mbox{ } \\ 
Prints the version of \stanc.  This is useful for bug reporting
and asking for help on the mailing lists.
%
\item[\tt --name={\slshape class\_name}]
\mbox{ } \\ 
Specify the name of the class used for the implementation of the
\Stan model in the generated \Cpp code.  
\\[6pt]
Default: {\tt {\slshape class\_name = anon\_model}}
%
\item[\tt -o={\slshape cpp\_file\_name}]
\mbox{ } \\ 
Specify the name of the file into which the generated \Cpp is written.
\\[6pt]
Default: {\tt {\slshape cpp\_file\_name} = {\slshape class\_name}.cpp}
%
\end{description}


\chapter{The C++ Model Class}

The generated \Cpp class extends a built-in \Stan abstract
base class for probability models.  Instances of the class are
constructed from a specified data vector $y$.  The data vector $y$
determines the dimensionality $K$ of the parameter vector $\theta$,
which in general may depend on size constants in $y$.  The class
implements a method that takes a parameter $K$-vector $\theta$ as
argument and returns the (unnormalized) total log probabilty,
\[
\theta 
\mapsto 
\log p(y,\theta) 
\]
The second method returns the gradient of the (unnormalized) total
probability as a function of a parameter $K$-vector $\theta$,
\[
\theta
\mapsto
\nabla_{\theta} \log p(y,\theta)
= ( \frac{\partial}{\partial\theta_1} \log p(y,\theta),
   \ldots, 
   \frac{\partial}{\partial\theta_K} \log p(y,\theta) ),
\]

The class computes gradients using accurate and efficient reverse-mode
algorithmic differentiaton.  The cost of computing the gradient is
a small multiple of the cost of computing the log probability.  The
cost inovlves a bounded amount of extra bookkeeping for each 
subexpression involved in computing the log probability.  Unlike
in the calculation of finite differences, the extra bookkeeping is
not dependent on the dimensionality of the parameter vector.



\chapter{Compiling C++ Programs}\label{compiling-cpp.chapter}

\Stan has been developed using two portable, open-source compilers,
\gpp and \clang, which run under Windows, Macintosh, and Unix/Linux.
\Stan has also been compiled using \MSVC, a Windows-specific compiler
from Microsoft.


\section{Which Compiler?}

It has been our experience that \clang is much faster to
compile at all optimization levels than \gpp, but that the code
generated by \gpp is slightly faster to execute.

\section{What the Compiler Does}

A \Cpp compiler like \gpp or \clang actually performs several
lower-level operations in sequence,
% 
\begin{enumerate}
\item
parsing the input \Cpp source file(s), 
\item 
generating relocatable object code, and
\item 
linking the relocatable object code into executable code.
\end{enumerate}
%
These stages may be called separately, though the examples in
this manual perform them in a single call.


\section{Including Library Code}

\Stan is written as a set of header-only libraries.  This simplifies
writing code that uses Stan.  The only thing that needs to be done
is to include the relevant libraries.  

The compiler command-line option to include a
header-only library is 
%
\begin{quote}
\code{-I {\slshape path-to-library}}.
\end{quote}
%
The path to the library must be such that any \code{\#include}
statements within the \Cpp source files be resolvable starting from
the path to the library.  

The header-only library code for \Stan itself is located in the
subdirectory \code{src/} of the top-level \Stan directory.  To allow
programs to use the \Stan library, the compiler needs to be given the
option 
%
\begin{quote}
\code{-I {\slshape stan}/src} 
\end{quote}
%
where \code{\slshape stan} is the
path to the top-level Stan directory.  If the compiler is called from the
top-level Stan directory, it suffices to use \code{-I src}, as in the
examples in the first chapter.

\Stan depends on two open-source libraries,
%
\begin{enumerate}
\item Boost general purpose \Cpp libraries, and 
\item Eigen matrix and linear algebra \Cpp libraries
\end{enumerate}
%
These are both distributed along with \Stan in the directory
\code{{\slshape stan}/lib/}, where again \code{\slshape stan} is the
top-level directory of the \Stan distribution.  Both libraries take
include paths starting under \code{lib/}.  For most uses of \Stan, it
is also necessary to include an explicit compiler option to include
Eigen and Boost,
%
\begin{quote}
\code{-I {\slshape stan}/lib}
\end{quote}
%
with \code{\slshape stan} being the location of the top-level \Stan
directory.  Thus calling \Stan typically requires all three of \Stan,
Boost, and Eigen to be included, which is accomplished with
%
\begin{quote}
\code{-I {\slshape stan}/lib -I {\slshape stan}/src}
\end{quote}
%
where \code{\slshape stan} is the path to the top-level \Stan directory.


\section{Compiler Optimization}

Stan was written with an optimizing compiler in mind.  For
that reason, it runs as much as an order of magnitude or more
faster with optimization turned on.  

For development, we recommend optimization level 0, whereas for
sampling, we recommend optimization level 3.  These are controlled
through the compiler option \code{-O} (capital letter `O').  To
generate efficient code, use
%
\begin{quote}
\code{-O3}
\end{quote}
%
where the first character is the capital letter `O'.
For faster compile time but less efficient code, use
%
\begin{quote}
\code{-O0}
\end{quote}
%
where the first character is the capital letter `O' and
the second character is the digit `0'.

\section{Executable Name}

If no name is provided for the executable, the default value of
\code{a.out} is used.  This executable will show up in the directory
from which the compiler was called.

To put the executable in a different location, the \code{-o {\slshape
    path-to-executable}} command may be used.  In an earlier example,
\code{-o bern} was used to write the executable to a file called
\code{bern}.  (In Windows, executables are suffixed with \code{.exe}.)



\chapter{Running a \Stan Program}\label{stan-cmd.chapter}

Once a \Stan program has been compiled (see \refchapter{stanc}), 









\chapter{Dump Data Format}\label{dump.chapter}

For representing structured data in files, \Stan uses the dump format
introduced in \SPLUS and used in \R and \JAGS (and in \BUGS, but with
a different ordering).   A dump file is structured as a sequence of
variable definitions.  Each variable is defined in terms of its
dimensionality and its values.   There are three kinds of variable
declarations, one for scalars, one for sequences, and one for general
arrays.

\section{Scalar Variables}

A simple scalar value can be thought of as having an empty list of
dimensions.  Its declaration in the dump format follows the \SPLUS
assignment syntax.  For example, the following would constitute a
valid dump file defining a single scalar variable \code{y} with value
17.2.
%
\begin{quote}
\begin{Verbatim}
y <- 
17.2
\end{Verbatim}
\end{quote}
%
A scalar value is just a zero-dimensional array value.

\section{Sequence Variables}

One-dimensional arrays may be specified directly using the \SPLUS
sequence notation.  The following example defines an integer-value and
a real-valued sequence.
%
\begin{quote}
\begin{Verbatim}
n <- c(1,2,3)
y <- c(2.0,3.0,9.7)
\end{Verbatim}
\end{quote}
%
It is possible to define an array without a declaration of
dimensionality because the reader just counts the number of entries to
determine the size of the array.

\section{Array Variables}

For more than one dimension, the dump format uses a dimensionality
specification.  For example,
%
\begin{quote}
\begin{verbatim}
y <- structure(c(1,2,3,4,5,6), .Dim = c(2,3))
\end{verbatim}
\end{quote}
%
This defines a $2 \times 3$ array.  Data is stored in column-major
order, meaning the values for \code{y} will be as follows.
%
\begin{quote}
\begin{Verbatim}
y[1,1] = 1     y[2,1] = 3     y[3,1] = 5    
y[2,1] = 2     y[2,2] = 4     y[3,2] = 6
\end{Verbatim}
\end{quote}
%
The \code{structure} keyword just wraps a sequence of values and a
dimensionality declaration, which is itself just a sequence of
non-negative integer values.  The product of the dimensions must equal
the length of the array.


\section{Integer- and Real-Valued Variables}

There is no declaration in a dump file that distinguishes integer
versus real values.  If a value in a dump file's definition of a
variable contains a decimal point, \Stan assumes that the values are
real.  If there are no decimal points in a variable's defined value, 
the value may be assigned to variables declared as integer or real
in \Stan.

The following dump file declares an integer value for \code{y}.
%
\begin{quote}
\begin{Verbatim} 
y <- 
2
\end{Verbatim}
\end{quote}
% 
This definition can be used for a \Stan variable \code{y} declared as
\code{real} or as \code{int}.  Assigning an integer value to a real
variable automatically promotes the integer value to a real value.

The following dump file provides a real value for \code{y}.
%
\begin{quote}
\begin{Verbatim}
y <-
2.0
\end{Verbatim}
\end{quote}
%
Even though this is a round value, the occurrence of the decimal
point in the value, \code{2.0}, causes \Stan to infer that \code{y} is
real valued.  This dump file may only be used for variables \code{y}
delcared as real in \Stan.


\section{Quoted Variable Names}

In order to support \JAGS data file, variables may be double quoted.
For instance, the following definition is legal in a dump file.
%
\begin{quote}
\begin{Verbatim}
"y" <-
c(1,2,3)
\end{Verbatim}
\end{quote}

\section{Line Breaks}

The line breaks in a dump file are required to be consistent with
the way \R reads in data.  Both of the following declarations are
legal.
%
\begin{quote}
\begin{Verbatim}
y <- 2
y <-
3
\end{Verbatim}
\end{quote}
%
Breaking before the assignment, as in \R, is not allowed.
%
\begin{quote}
\begin{Verbatim}
y
<- 2
\end{Verbatim}
\end{quote}

Lines may also be broken in the middle of sequences declared
using the \code{c(...)} notation., as well as between the comma
following a sequence definition and the dimensionality declaration.
For example, the following declaration of a $2 \times 3 \times 4$
array is valid.
%
\begin{quote}
\begin{Verbatim}
y <-
structure(c(1,2,3,
4,5,6,7,8,9,10,11,
12), .Dim = c(2,3,
4))
\end{Verbatim}
\end{quote}

\section{General R Sequence Syntax}

Sometimes, \R will use shorthand for its output. For example,
starting \R,

\begin{quote}
\begin{Verbatim}[fontshape=sl]
> R
\end{Verbatim}
\end{quote}
%
and then within \R, executing the following commands,

\begin{quote}
\begin{Verbatim}[fontshape=sl]
R> e <- matrix(c(1,2,3,4,5,6),nrow=2,ncol=3)
R> dump("e")
\end{Verbatim}
\end{quote}
%
leads to a \code{dumpdata.R} file being created with
the following contents.
%
\begin{quote}
\begin{Verbatim}
e <-
structure(c(1, 2, 3, 4, 5, 6), .Dim = 2:3)
\end{Verbatim}
\end{quote}
%
\R has used the fact that it allows a contiguous
sequence to be specified with its start and end point
using the notation \code{2:3}.  \Stan cannot currently
parse this format of input.  

Alternatively, \R is prone to include long-integer specifiers.
For instance, a $2 \times 2$ matrix is dumped as follows.
%
\begin{quote}
\begin{Verbatim}
f <-
structure(c(1, 2, 3, 4), .Dim = c(2L, 2L))
\end{Verbatim}
\end{quote}
%
Here the dimensions are defined to be \code{c(2L,~2L)}.  \Stan 
always treats these dimesions as \code{L}.




\chapter{Variable Transforms}

To avoid having to deal with constraints while simulating the
Hamiltonian dynamics during sampling, every (multivariate) parameter
in a \Stan model is transformed to an unconstrained variable behind
the scenes by the model compiler.  The transform is based on any
constraints in the parameter's definition.  Constraints that may be
placed on variables include upper and lower bounds, positive ordered
vectors, simplex vectors, correlation matrices and covariance
matrices.  This chapter provides a definition of the transforms used
for each type of variable.

Once the model is compiled, it has support on all of
$\mathbb{R}^K$, where $K$ is the number of unconstrained parameters
needed to define the actual parameters defined in the model.

The details of section need not be understood in order to use
\Stan for well-behaved models.  Understanding the sampling behavior
of \Stan fully requires understanding these transforms.


\section{Changes of Variables}

The support of a random variable $X$ with density $p_X(x)$ is that
subset of values for which it has non-zero density,
%
\[
\mbox{support}(X) = \{ x | p_X(x) > 0 \}.
\]

If $f$ is a total function defined on the support of $X$, then $Y =
f(X)$ is a new random variable.  This section shows how to compute the
probability density function of $Y$ for a few useful and well-behaved
transforms $f$.


\subsection{Univariate Changes of Variables}

Suppose $X$ is one dimensional and $f: \mbox{support}(X) \rightarrow
\mathbb{R}$ is a one-to-one, monotonic function with a differentiable
inverse $f^{-1}$.  Then the density of $Y$ is given by
%
\[
p_Y(y) = p_X(f^{-1}(y))  
         \,
         \left| \, \frac{d}{dy} f^{-1}(y)\, \right|.
\]


\subsection{Multivariate Changes of Variables}


Suppose $X$ is a $K$-dimensional random variable with probability
density function $p_X(x)$.  A new random variable $Y = f(X)$ may be
defined by transforming $X$ with a suitably well-behaved function $f$.
It suffices for what follows to note that if $f$ is one-to-one
and its inverse $f^{-1}$ has a well-defined Jacobian, then the
density of $Y$ is
%
\[
p_Y(y) = p_X(g(y)) \, \left| \, \det \, J_g(y) \, \right|,
\]
%
where $\det{}$ is the matrix determinant operation and $J_{f^{-1}}(y)$ is
the Jacobian of $f^{-1}$ evaluated at $y$.  The latter is defined by
\[
J_{f^{-1}}(y) = 
\left[
\begin{array}{ccc}\displaystyle
\frac{\partial y_1}{\partial x_1}
& \cdots
& \displaystyle \frac{\partial y_1}{\partial x_{K}}
\\[6pt]
\vdots & \vdots & \vdots
\\
\displaystyle\frac{\partial y_{K}}{\partial x_1}
& \cdots
& \displaystyle\frac{\partial y_{K}}{\partial x_{K}}
\end{array}
\right].
\]
%
If the Jacobian is a triangular matrix, the determinant reduces to the
product of the diagonal entries,
%
\[
\det \, J_{f^{-1}}(y)
= \prod_{k=1}^K \frac{\partial y_k}{\partial x_k}.
\]
%
Triangular matrices naturally arise in situations where the variables
are ordered, for instance by dimension, and each variable's
transformed value depends on the previous variable's transformed
values.  Diagonal matrices, a simple form of triangular matrix,
arise if each transformed variable only depends on a single raw
variable.

\section{Lower Bounds}

\Stan uses a logarithmic transform for lower and upper bounds.  If a
variable $X$ is declared to have lower bound $c$, it is transformed to
a new variable $Y$, where
%
\[
Y = \log(X - c),
\]
%
the inverse transform is
%
\[
f^{-1}(y) = \exp(y) + c,
\]
%
and the density of $Y$ is
%
\begin{eqnarray*}
p_Y(y) 
& = & p_X\!\left( \exp(y) + c \right) \cdot \left| \frac{d}{dy} \left(
    \exp(y) + c \right) \right|
\\[6pt]      
& = & p_X\!\left( \exp(y) + c \right) \cdot \exp(y).
\end{eqnarray*}


\section{Upper Bounds}

\Stan treats upper bounds in the same way as lower bounds, with a
logarithmic transform.  Only this time the result must be negated.  If
a variable $X$ is declared to have an upper bound $d$, it is
transformed to a new variable $Y$, where
%
\[
Y = \log(d - X)
\]
%
the inverse transform is
%
\[
f^{-1}(y) = d - \exp(y)
\]
%
and the density of $Y$ is
%
\begin{eqnarray*}
p_Y(y) 
& = & p_X \!\left( d - \exp(y) \right) \cdot \left| \frac{d}{dy} \left( d
    - \exp(y) \right) \right|
\\[6pt]      
& = &  p_X \!\left( d - \exp(y) \right) \cdot \exp(y).
\end{eqnarray*}


\section{Lower and Upper Bounds}

For lower and upper-bounded variables, \Stan uses the log-odds
transform.  It scales and translates from the basic log-odds function
defined for $u \in (0,1)$ by
%
\[
\mbox{logit}(u) = \log \frac{u}{1 - u}.
\]
% 
The inverse of the log odds function is the logistic sigmoid, defined 
for $v \in (-\infty,\infty)$ by
%
\[
\mbox{logit}^{-1}(v) = \frac{1}{1 + \exp(-v)}.
\]
% 
The derivative of the logistic sigmoid is
%
\[
\frac{d}{dy} \mbox{logit}^{-1}(y) 
= \mbox{logit}^{-1}(y) \cdot \left( 1 - \mbox{logit}^{-1}(y) \right).
\]
 
For variables constrained to be in the open interval $(a,b)$, \Stan
uses a scaled and translated log-odds transform.  If variable $X$ is
declared to have lower bound $a$ and upper bound $b$, then it is
transformed to a new variable $Y$, where
%
\[
Y = \mbox{logit} \left( \frac{X - a}{b - a} \right).
\]
%
The inverse of this transform is
%
\[
X = a + (b - a) \cdot \mbox{logit}^{-1}(Y)
\]
%
and the density of the transformed variable is
%
\begin{eqnarray*}
p_Y(y) 
& = & p_X \! \left( a + (b - a) \cdot \mbox{logit}^{-1}(y) \right)
    \left|  \frac{d}{dy} a + (b - a) \cdot \mbox{logit}^{-1}(y)
    \right| .
\\[8pt]
& = & p_X \! \left( a + (b - a) \cdot \mbox{logit}^{-1}(y) \right)
    \cdot (b - a)
    \cdot \mbox{logit}^{-1}(y)
    \cdot \left( 1 - \mbox{logit}^{-1}(y) \right).
\end{eqnarray*}
%
Despite its apparent complexity, $\mbox{logit}^{-1}(y)$, and hence
$\exp(-y)$, need only be evaluated once.


\section{Positive Ordered}

For some modeling tasks, a vector-valued random variable $X$ is
required with support on positive, ordered sequences.  One example is
the set of cut points in ordinal logistic regression.  

In constraint terms, a positive, ordered vector $x \in \mathbb{R}^K$
is a vector that satisfies
\[
0 < x_1
\]
%
and for $2 \leq k \leq K$,
\[
x_{k-1} < x_k
\]
%

\Stan's transform follows the constraint directly.  It maps a vector
$x \in \mathbb{R}^{K}$ to a vector $f(x) = y \in \mathbb{R}^K$ by setting
%
\[
y_1 = f_1(x) = \log x_1
\] 
%
and for $2 \leq k \leq K$,
\[
y_k = f_k(x) = \log \left( x_{k} - x_{k-1} \right).
\]

The inverse transform $x = f^{-1}(y)$ satisfies
%
\[
x_1 = \exp(y_1)
\]
%
and for $2 \leq k \leq K$, 
\[
x_k = x_{k-1} + \exp(y_k) = \sum_{k' =1}^{k} \exp(y_{k'}).
\]

The Jacobian of the inverse transform $f^{-1}$ is lower triangular,
with diagonal elements
\[
J_{1,1} = \frac{\partial}{\partial y_1} f_1^{-1}(y) = \exp(y_1)
\]
and for $2 \leq k \leq K$,
\[
J_{k,k} = \frac{\partial}{\partial y_k} f_k^{-1}(y) = \exp(y_k).
\]
%
Because of the triangularity and the positivivity of the $x_k$ and
their differences, the absolute determinant of the Jacobian is
%
\[
\left| \, \det\, J \, \right|
= \prod_{k=1}^K \exp(y_k)
\]


Putting this all together, if $p_X$ is the density of $X$, then the
transformed variable $Y$ has density $p_Y$ given by
%
\[
p_Y(y)
= p_X(f^{-1}(y)) 
\
\prod_{k=1}^K \exp(y_k).
\]


\section{Unit Simplex}

The parameter of the $K$-dimensional categorical distribution must lie
in the $K$-simplex.  Consequently, simplex-constrained variables show
up in multivariate discrete models of all kinds.  

The $K$-simplex is the set of points $x \in \mathbb{R}^K$ such that
for $1 \leq k \leq K$, 
\[ 
x_k > 0,
\] 
and
\[
\sum_{k=1}^K x_k = 1.
\]
%   

A point $x$ in the $K$-simplex is fully determined by its first $K-1$
dimensions, because rearranging terms in the constraint yields
%
\[
x_K = 1 - \sum_{k=1}^{K-1} x_k.
\]
%

Stan employs a transform whose inverse may be understood using a
stick-breaking metaphor.  A simplex is determined by taking a stick of
unit length, breaking a piece off, the length of which is $x_1$.  Then
$x_2$ is determined by breaking a piece from what's left.  A total of
$K-1$ pieces are broken off, determining $x_1,\ldots,x_{K-1}$.  To
complete the metaphor, the length of the remaining piece after $K-1$
pieces are broken off determines $x_K$.

The simplex transform $f$ is most easily understood in terms of its
inverse $x = f^{-1}(y)$, which maps a point in $y \in
\mathbb{R}^{K-1}$ to a point $x$ in the $K$-simplex.  An intermediate
vector $z \in \mathbb{R}^{K-1}$, whose coordinates $z_k$ represent 
the proportion of the stick broken off in step $k$, is defined
elementwise for $1 \leq k < K$ by
%
\[
z_k = \mbox{logit}^{-1} \left( y_k 
                             - \mbox{logit} \left( \frac{1}{K - k + 1}
                                            \right)
                       \right).
\]
%
The logit term in the above definition adjusts the transform so that a
zero vector $y$ is mapped to $(1/K,\ldots,1/K)$.  For instance, if
$y_1 = 0$, then $z_1 = 1/K$; if $y_2 = 0$, then $z_2 = 1/(K-1)$; and
if $z_{K-1} = 0$, then $z_{K-1} = 1/2$.  This ensures that random
initializations for categorical distribution parameters are
initialized around a parameter value when $y = 0$ representing the
uniform distribution.

The break proportions $z$ are applied to determine the stick sizes and
resulting value of $x_k$ for $1 \leq k < K$ by
%
\[
x_k = 
\left( 1 - \sum_{k'=1}^{k-1} x_{k'} \right) z_k.
\]
%
The summation term represents the length of stick left at stage $k$.
This is multiplied by the break proportion $z_k$ to yield $x_k$.
Because $x$ lines in a $K$-simplex, $x_K$ is determined from
$x_1,\ldots,x_{K-1}$.

The Jacobian $J$ of the inverse transform $f^{-1}$ is
lower-triangular, with diagonal entries
\[
J_{k,k}
=
\frac{\partial x_k}{\partial y_k}
=
\frac{\partial x_k}{\partial z_k} \,
\frac{\partial z_k}{\partial y_k},
\]
%
where
\[
\frac{\partial z_k}{\partial y_K} 
= \frac{\partial}{\partial z_k} 
   \mbox{logit}^{-1} \left(
                       y_k - \mbox{logit} \left( \frac{1}{K-k+1}
                                          \right)
                    \right)
= z_k (1 - z_k),
\]
%
and
%
\[
\frac{\partial x_k}{\partial z_k}
=
\left( 
  1 - \sum_{k' = 1}^{k-1} x_{k'}
   \right)
.
\]
%
Note that the definition is recursive, definining $x_k$ in terms of
$x_{k'}$ where $k' < k$.

Because the Jacobian $J$ of $f^{-1}$ is lower triangular and positve, its
absolute determinant reduces to
%
\[
\left| \, \det J \, \right|
\ = \
\prod_{k=1}^{K-1} J_{k,k}
\ = \
\prod_{k=1}^{K-1} 
z_k
\, 
(1 - z_k)
\
\left(
1 - \sum_{k'=1}^{k-1} x_{k'}
\right)
.
\]
%
Thus the transformed variable $Y = f(X)$ has a density given by
%
\[
p_Y(y) 
= p_X(f^{-1}(y))
\,
\prod_{k=1}^{K-1} 
z_k
\, 
(1 - z_k)
\
\left(
1 - \sum_{k'=1}^{k-1} x_{k'}
\right)
.
\]
%
This formula looks more complicated than it is.  It only involves a
single exponential function evaluation involved (in the logistic
sigmoid applied to $y_k$ to produce $z_k$);  everything else is just
basic arithmetic and keeping track of the remaing stick length.

The transform $Y = f(X)$ can be derived by reversing the stages of the
inverse transform.  Working backwards, given the break proportions
$z$, $y$ is defined elementwise by
%
\[
y_k 
= \mbox{logit}(z_k)
+ \mbox{logit}\left(
   \frac{1}{K-k+1}
   \right)
.
\]
%
The break proportions $z_k$ are defined to be the ratio of $x_k$ to
the length of stick left after the first $k-1$ pieces have been broken
off, 
%
\[
z_k 
= \frac{x_k}
       {1 - \sum_{k' = 1}^{k-1} x_k}
.
\]






\chapter{Modeling Language Reference}\label{model-ref.chapter}

\Stan's modeling language is more procedural than what users may be
familiar with from BUGS and JAGS, both of which are declarative.  For
instance, \Stan statements are executed in the order they are written
in model specifications and local variables may be reassigned as in a
procedural programming language.  Furthermore, variables must be
declared before they are use.  

Like most programming languages, \Stan is defined syntactically in
terms of expressions, which denote values, and statements, which
denote an action to be taken.

The purpose of \Stan's modeling language is to allow users to write
down a probability function up to a multiplicative normalizing
constant.  


\section{Expressions}

The top-level grammar for expressions is provided in
\reffigure{expression-grammar}.

\begin{figure}
\begin{center}
{%\footnotesize
\begin{Verbatim}
expression ::= literal
             | variable
             | expression infixOp expression
             | prefixOp expression
             | expression postfixOp
             | expression '[' expressions ']'
             | function '(' expressions ')'
             | '(' expression ')'
\end{Verbatim}
}
\end{center}
\mycaption{expression-grammar}
          {The top-level expression grammar for \Stan model specifications.}
\end{figure}

Valid \Stan expresions include simple numerical literals (e.g.,
\code{2}, \code{32.7}), identifiers representing variables (e.g.,
\code{theta}), 

The simplest form of expression is a literal denoting a
value, such as \code{32.7}.  Expressions may also consist of


\chapter{Optimizing \Stan Code}\label{optimization.chapter}




\appendix

\chapter{Installation}\label{install.appendix}

\chapter*{References}

\end{document}