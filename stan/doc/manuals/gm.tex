\documentclass[11pt]{report}

\usepackage{times}

\usepackage{xspace}

\newcommand{\Stan}{Stan\xspace}
\newcommand{\stanc}{stanc\xspace}
\newcommand*{\Cpp}{C{\ensuremath{++}}\xspace}


\title{\Stan Modeling Language Reference Manual}
\author{\Stan Development Team}
\date{\footnotesize \today}

\begin{document}

\maketitle

\begin{abstract}
  \Stan's modeling language allows users to write a model specifying a
  probability function $p(y,\theta)$, where $y$ is for known values
  (e.g., constants, hyperparameters, and modeled data) and $\theta$ is
  for unknown values (e.g., estimated parameters, missing data, or
  simulated values).

  The stanc compiler converts a Stan program to a \Cpp class.
  Instances of the class are constructed given values for
  $y$.  The class provides methods to compute the log probability,
  $\log p(y,\theta)$, and its gradient, $\nabla_{\theta} \log
  p(y,\theta)$, as functions of $\theta$.  Gradients are computed using
  accurate and efficient reverse-mode algorithmic differentiaton at a
  small multiple of the cost of the log probability function itself
  (independently of dimensionality).
  
  The C++ class can be plugged into Stan's continuous and discrete
  samplers to draw a sequence of samples $\theta^{(m)}$ from the
  posterior $p(\theta|y) \propto p(y,\theta)$.  These samples may be
  used for full Bayesian inference, much of which can be done directly
  within Stan.

  For continuous variables, Stan uses Hamiltonian Monte Carlo (HMC)
  sampling.  Stan implements the No-U-Turn Sampler (NUTS), an adaptive
  version of HMC that automatically tunes step sizes and a diagonal
  mass matrix during warmup and then adapts the number of leapfrog
  integration steps during sampling.  Stan is expressive enough to
  allow most discrete variables to be marginalized out.  For the remaining
  discrete parameters, Stan uses Gibbs sampling if there are only a few
  outcomes and adaptive slice sampling otherwise.

  Stan's language is more expressive than that of its predecessors,
  BUGS and JAGS.  Most strikingly, variables and expressions are
  strongly typed and declared as data or parameters in the model.
  Stan also supports a broader range of arithmetic, matrix, and linear
  algebra operations than BUGS or JAGS.  Users may manipulate
  log probability functions directly and are not required to use
  proper priors.

  % Stan also supports optimization methods that are able to find
  % a $\theta^{*}$ that (locally) optimizes $p(\theta|y)$.
\end{abstract}

\chapter{Getting Started}

Here's a simple example of a Stan model for estimating a simple
Bernoulli parameter.

\begin{verbatim}
data {
    int(0,) N;
    int(0,1) y[N];
}
parameters {
    real theta;
}
model {
    for (n in 1:N)
        y[n] ~ bernoulli(theta);
}
\end{verbatim}


\end{document}