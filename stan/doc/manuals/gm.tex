\documentclass[11pt]{report}

\usepackage{xspace}
\usepackage{times}

\newcommand{\Stan}{Stan\xspace}
\newcommand{\stanc}{stanc\xspace}
\newcommand*{\Cpp}{C{\ensuremath{++}}\xspace}


\title{\Stan Modeling Language Reference Manual}
\author{\Stan Development Team}
\date{\footnotesize \today}

\begin{document}

\maketitle

\begin{abstract}
  \Stan's modeling language allows users to write a model specifying a
  probability function $p(y,\theta)$ with variables $y$ and $\theta$
  corresponding to known and unknown values.  These values are
  typically used for data (including size constants and fixed
  paramters) and parameters (including missing data and posterior
  simulations) in a Bayesian model.

  The stanc compiler converts a Stan program to a \Cpp class.
  Instances of the class are constructed from known values for
  $y$.  The class provides methods to compute the log probability,
  $\log p(y,\theta)$, and its gradient, $\nabla_{\theta} \log
  p(y,\theta)$, as functions of $\theta$.  Gradients are computed using
  accurate and efficient reverse-mode algorithmic differentiaton at a
  small multiple of the cost of the log probability function itself
  (independently of dimensionality).
  
  The C++ class can be plugged into Stan's continuous and discrete
  samplers to draw a sequence of samples $\theta^{(m)}$ from the
  posterior $p(\theta|y) \propto p(y,\theta)$.  These samples may be
  used for full Bayesian inference, much of which can be done directly
  within Stan.

  For continuous variables, Stan uses Hamiltonian Monte Carlo (HMC)
  sampling.  Stan implements the No-U-Turn Sampler (NUTS), an adaptive
  version of HMC that automatically tunes step sizes and a diagonal
  mass matrix during warmup and then adapts the number of leapfrog
  integration steps during sampling.  Stan is expressive enough to
  allow most discrete variables to be marginalized out.  For the remaining
  discrete parameters, Stan uses Gibbs sampling if there are only a few
  outcomes and adaptive slice sampling otherwise.

  Stan's language is more expressive than that of its predecessors,
  BUGS and JAGS.  Variables and expressions are strongly typed.  Stan
  supports many more arithmetic, matrix, and linear algebra operations
  than its predecessors.  Users may also manipulate log probability
  functions directly.

  % Stan also supports optimization methods that are able to find
  % a $\theta^{*}$ that (locally) optimizes $p(\theta|y)$.
\end{abstract}

\chapter{Getting Started}

Here's a simple example of a Stan model for estimating a simple
Bernoulli parameter.

{\footnotesize
\begin{verbatim}
data {
    int(0,) N;
    int(0,1) y[N];
}
parameters {
    real theta;
}
model {
    for (n in 1:N)
        y[n] ~ bernoulli(theta);
}
\end{verbatim}}


\end{document}